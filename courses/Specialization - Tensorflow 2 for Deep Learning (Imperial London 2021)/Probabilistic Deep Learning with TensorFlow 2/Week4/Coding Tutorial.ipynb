{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Coding Tutorial.ipynb","provenance":[],"collapsed_sections":["PjypvWoRdchx"]}},"cells":[{"cell_type":"code","metadata":{"id":"g4-75HtedchR","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n","tfd = tfp.distributions\n","tfpl = tfp.layers\n","tfb = tfp.bijectors\n","print(\"TF version:\", tf.__version__)\n","print(\"TFP version:\", tfp.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gIIgKsbrdchV","colab_type":"text"},"source":["# Variational autoencoders"]},{"cell_type":"markdown","metadata":{"id":"EVv0KHEddchV","colab_type":"text"},"source":["## Coding tutorials\n","\n","#### [1. Encoders and decoders](#tutorial1)\n","#### [2. Minimising Kullback-Leibler divergence](#tutorial2)\n","#### [3. Maximising the ELBO](#tutorial3)\n","#### [4. KL divergence layers](#tutorial4)"]},{"cell_type":"markdown","metadata":{"id":"09n3i-9ddchW","colab_type":"text"},"source":["***\n","<a class=\"anchor\" id=\"tutorial1\"></a>\n","## Encoders and decoders"]},{"cell_type":"code","metadata":{"id":"I0xDavBddchW","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten, Reshape\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TxT0ss2dchY","colab_type":"code","colab":{}},"source":["# Load Fashion MNIST\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","x_train = x_train.astype('float32')/255.\n","x_test = x_test.astype('float32')/255.\n","class_names = np.array(['T-shirt/top', 'Trouser/pants', 'Pullover shirt', 'Dress',\n","                        'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag','Ankle boot'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ih7xvEqddchb","colab_type":"code","colab":{}},"source":["# Display a few examples\n","\n","n_examples = 1000\n","example_images = x_test[0:n_examples]\n","example_labels = y_test[0:n_examples]\n","\n","f, axs = plt.subplots(1, 5, figsize=(15, 4))\n","for j in range(len(axs)):\n","    axs[j].imshow(example_images[j], cmap='binary')\n","    axs[j].axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSL-ufv1dchd","colab_type":"code","colab":{}},"source":["# Define the encoder\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGXj2WDsdchf","colab_type":"code","colab":{}},"source":["# Encode examples before training\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWEdZUoCdchi","colab_type":"code","colab":{}},"source":["# Plot encoded examples before training \n","\n","f, ax = plt.subplots(1, 1, figsize=(7, 7))\n","sns.scatterplot(pretrain_example_encodings[:, 0],\n","                pretrain_example_encodings[:, 1],\n","                hue=class_names[example_labels], ax=ax,\n","                palette=sns.color_palette(\"colorblind\", 10));\n","ax.set_xlabel('Encoding dimension 1'); ax.set_ylabel('Encoding dimension 2')\n","ax.set_title('Encodings of example images before training');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPhtUPGtdchk","colab_type":"code","colab":{}},"source":["# Define the decoder\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mv7fb-Gldchm","colab_type":"code","colab":{}},"source":["# Compile and fit the model\n","\n","\n","\n","# Specify loss - input and output is in [0., 1.], so we can use a binary cross-entropy loss\n","\n","\n","# Fit model - highlight that labels and input are the same\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oglGroMwdcho","colab_type":"code","colab":{}},"source":["# Compute example encodings after training\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiAJEnfpdchq","colab_type":"code","colab":{}},"source":["# Compare the example encodings before and after training\n","\n","f, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n","sns.scatterplot(pretrain_example_encodings[:, 0],\n","                pretrain_example_encodings[:, 1],\n","                hue=class_names[example_labels], ax=axs[0],\n","                palette=sns.color_palette(\"colorblind\", 10));\n","sns.scatterplot(posttrain_example_encodings[:, 0],\n","                posttrain_example_encodings[:, 1],\n","                hue=class_names[example_labels], ax=axs[1],\n","                palette=sns.color_palette(\"colorblind\", 10));\n","\n","axs[0].set_title('Encodings of example images before training');\n","axs[1].set_title('Encodings of example images after training');\n","\n","for ax in axs: \n","    ax.set_xlabel('Encoding dimension 1')\n","    ax.set_ylabel('Encoding dimension 2')\n","    ax.legend(loc='lower right')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvkJOyxVdcht","colab_type":"code","colab":{}},"source":["# Compute the autoencoder's reconstructions\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evcgHHESdchv","colab_type":"code","colab":{}},"source":["# Evaluate the autoencoder's reconstructions\n","\n","f, axs = plt.subplots(2, 5, figsize=(15, 4))\n","for j in range(5):\n","    axs[0, j].imshow(example_images[j], cmap='binary')\n","    axs[1, j].imshow(reconstructed_example_images[j].numpy().squeeze(), cmap='binary')\n","    axs[0, j].axis('off')\n","    axs[1, j].axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjypvWoRdchx","colab_type":"text"},"source":["***\n","<a class=\"anchor\" id=\"tutorial2\"></a>\n","## Minimising Kullback-Leibler divergence"]},{"cell_type":"code","metadata":{"id":"kSLPAc7adchy","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XhLH8btddch0","colab_type":"code","colab":{}},"source":["# Define a target distribution, p\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qK-EbikSdch1","colab_type":"code","colab":{}},"source":["# Plot the target distribution's density contours\n","\n","def plot_density_contours(density, X1, X2, contour_kwargs, ax=None):\n","    '''\n","        Plots the contours of a bivariate TensorFlow density function (i.e. .prob()).\n","        X1 and X2 are numpy arrays of mesh coordinates.\n","    '''\n","    X = np.hstack([X1.flatten()[:, np.newaxis], X2.flatten()[:, np.newaxis]])\n","    density_values = np.reshape(density(X).numpy(), newshape=X1.shape)\n","    \n","    if ax==None:\n","        _, ax = plt.subplots(figsize=(7, 7))\n","    \n","    ax.contour(X1, X2, density_values, **contour_kwargs)\n","    return(ax)\n","\n","x1 = np.linspace(-5, 5, 1000)\n","x2 = np.linspace(-5, 5, 1000)\n","X1, X2 = np.meshgrid(x1, x2)\n","f, ax = plt.subplots(1, 1, figsize=(7, 7))\n","\n","# Density contours are linearly spaced\n","contour_levels = np.linspace(1e-4, 10**(-0.8), 20) # specific to this seed\n","ax = plot_density_contours(p.prob, X1, X2, \n","                           {'levels':contour_levels, \n","                            'cmap':'cividis'}, ax=ax)\n","ax.set_xlim(-5, 5); ax.set_ylim(-5, 5); \n","ax.set_title('Density contours of target distribution, $p$')\n","ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRw3R8Vjdch3","colab_type":"code","colab":{}},"source":["# Initialize an approximating distribution, q, that has diagonal covariance\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ak7QtihPdch6","colab_type":"code","colab":{}},"source":["# Define a function for the Kullback-Leibler divergence\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMGf2ULkdch8","colab_type":"code","colab":{}},"source":["# Run a training loop that computes KL[q || p], updates q's parameters using its gradients\n","\n","num_train_steps = 250\n","opt = tf.keras.optimizers.Adam(learning_rate=.01)\n","for i in range(num_train_steps):\n","    \n","    # Compute the KL divergence and its gradients\n","    q_loss, grads = loss_and_grads(q, p)\n","    \n","    # Update the trainable variables using the gradients via the optimizer\n","    opt.apply_gradients(zip(grads, q.trainable_variables))\n","    \n","    # Plot the updated density \n","    if ((i + 1) % 10 == 0):\n","        clear_output(wait=True)\n","        ax = plot_density_contours(p.prob, X1, X2,\n","                                   {'levels':contour_levels,\n","                                    'cmap':'cividis', 'alpha':0.5})\n","        ax = plot_density_contours(q.prob, X1, X2, \n","                                   {'levels':contour_levels,\n","                                    'cmap':'plasma'}, ax=ax)\n","        ax.set_title('Density contours of $p$ and $q$\\n' +\n","                     'Iteration ' + str(i + 1) + '\\n' +\n","                      '$D_{KL}[q \\ || \\ p] = ' + \n","                      str(np.round(q_loss.numpy(), 4)) + '$',\n","                      loc='left')\n","        plt.pause(.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOOGe_O2dch-","colab_type":"code","colab":{}},"source":["# Re-fit the distribution, this time fitting q_rev by minimising KL[p || q_rev]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKb0BBSJdciA","colab_type":"code","colab":{}},"source":["# Edit loss_and_grads function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9FAAr_RdciC","colab_type":"code","colab":{}},"source":["# Re-initialize optimizer, run training loop\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=.01)\n","for i in range(num_train_steps):\n","    # Reverse the KL divergence terms - compute KL[p || q_rev] \n","    q_rev_loss, grads = loss_and_grads(q_rev, p, reverse=True)\n","    \n","    # Update the trainable variables using the gradients via the optimizer\n","    opt.apply_gradients(zip(grads, q_rev.trainable_variables))\n","    \n","    # Plot the updated density \n","    if ((i + 1) % 10 == 0):\n","        clear_output(wait=True)\n","        ax = plot_density_contours(p.prob, X1, X2,\n","                                   {'levels':contour_levels,\n","                                    'cmap':'cividis', 'alpha':0.5})\n","        ax = plot_density_contours(q_rev.prob, X1, X2, \n","                                   {'levels':contour_levels,\n","                                    'cmap':'plasma'}, ax=ax)\n","        ax.set_title('Density contours of $p$ and $q_{rev}$\\n' +\n","                     'Iteration ' + str(i + 1) + '\\n' +\n","                      '$D_{KL}[p \\ || \\ q_{rev}] = ' + \n","                      str(np.round(q_rev_loss.numpy(), 4)) + '$',\n","                      loc='left')\n","        plt.pause(.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5ho0MGDdciD","colab_type":"code","colab":{}},"source":["# Plot q and q_rev alongside one another\n","\n","f, axs = plt.subplots(1, 2, figsize=(15, 7))\n","\n","axs[0] = plot_density_contours(p.prob, X1, X2,\n","                           {'levels':contour_levels,\n","                            'cmap':'cividis', 'alpha':0.5}, ax=axs[0])\n","axs[0] = plot_density_contours(q.prob, X1, X2, \n","                           {'levels':contour_levels,\n","                            'cmap':'plasma'}, ax=axs[0])\n","axs[0].set_title('Density contours of $p$ and $q$\\n' +\n","              '$D_{KL}[q \\ || \\ p] = ' + str(np.round(q_loss.numpy(), 4)) + '$',\n","              loc='left')\n","\n","axs[1] = plot_density_contours(p.prob, X1, X2,\n","                           {'levels':contour_levels,\n","                            'cmap':'cividis', 'alpha':0.5}, ax=axs[1])\n","axs[1] = plot_density_contours(q_rev.prob, X1, X2, \n","                           {'levels':contour_levels,\n","                            'cmap':'plasma'}, ax=axs[1])\n","axs[1].set_title('Density contours of $p$ and $q_{rev}$\\n' +\n","              '$D_{KL}[p \\ || \\ q_{rev}] = ' + str(np.round(q_rev_loss.numpy(), 4)) + '$',\n","              loc='left');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EhFM0T-zdciF","colab_type":"text"},"source":["---\n","<a class=\"anchor\" id=\"tutorial3\"></a>\n","## Maximising the ELBO \n","\n","Review of terminology:\n","- $p(z)$ = prior\n","- $q(z|x)$ = encoding distribution\n","- $p(x|z)$ = decoding distribution\n","\n","\\begin{align}\n","\\log p(x) &\\geq \\mathrm{E}_{Z \\sim q(z | x)}\\big[−\\log q(Z | x) + \\log p(x, Z)\\big]\\\\\n","          &= - \\mathrm{KL}\\big[ \\ q(z | x) \\ || \\ p(z) \\ \\big] + \\mathrm{E}_{Z \\sim q(z | x)}\\big[\\log p(x | Z)\\big] \n","\\end{align}"]},{"cell_type":"code","metadata":{"id":"8CIOHfBldciG","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten, Reshape\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEFGuHSadciI","colab_type":"code","colab":{}},"source":["# Import Fasion MNIST, make it a TensorFlow Dataset\n","\n","(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n","x_train = x_train.astype('float32')/255.\n","x_test = x_test.astype('float32')/255.\n","example_x = x_test[:16]\n","\n","batch_size = 64\n","x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7IzGEXvdciK","colab_type":"code","colab":{}},"source":["# Define the encoding distribution, q(z|x)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aw6H691LdciN","colab_type":"code","colab":{}},"source":["# Pass an example image through the network - should return a batch of MultivariateNormalDiags.\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5gVXmYtdciP","colab_type":"code","colab":{}},"source":["# Define the decoding distribution, p(x|z)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9d-cE6C5dciR","colab_type":"code","colab":{}},"source":["# Pass a batch of examples to the decoder\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JV7lcvMAdciT","colab_type":"code","colab":{}},"source":["# Define the prior, p(z) - a standard bivariate Gaussian\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FiMwjo4ddciW","colab_type":"text"},"source":["The loss function we need to estimate is\n","\n","\\begin{equation}\n","-\\mathrm{ELBO} = \\mathrm{KL}[ \\ q(z|x) \\ || \\ p(z) \\ ] - \\mathrm{E}_{Z \\sim q(z|x)}[\\log p(x|Z)]\\\\\n","\\end{equation}\n","\n","where $x = (x_1, x_2, \\ldots, x_n)$ refers to all observations, $z = (z_1, z_2, \\ldots, z_n)$ refers to corresponding latent variables.\n","\n","Assumed independence of examples implies that we can write this as\n","\n","\\begin{equation}\n","\\sum_j \\mathrm{KL}[ \\ q(z_j|x_j) \\ || \\ p(z_j) \\ ] - \\mathrm{E}_{Z_j \\sim q(z_j|x_j)}[\\log p(x_j|Z_j)]\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"REYFiGDsdciW","colab_type":"code","colab":{}},"source":["# Specify the loss function, an estimate of the -ELBO\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4qIqCLrdciY","colab_type":"code","colab":{}},"source":["# Define a function that returns the loss and its gradients\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cWbgIxZdcia","colab_type":"code","colab":{}},"source":["# Compile and train the model\n","\n","num_epochs = 5\n","opt = tf.keras.optimizers.Adam()\n","for i in range(num_epochs):\n","    for train_batch in x_train:\n","        current_loss, grads = get_loss_and_grads(train_batch)\n","        opt.apply_gradients(zip(grads, encoder.trainable_variables\n","                                       + decoder.trainable_variables))\n","        \n","    print('-ELBO after epoch {}: {:.0f}'.format(i+1, current_loss.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iU-YvB81dcic","colab_type":"code","colab":{}},"source":["# Connect encoder and decoder, compute a reconstruction\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TK0TJrmMdcie","colab_type":"code","colab":{}},"source":["# Plot examples against reconstructions\n","\n","f, axs = plt.subplots(2, 6, figsize=(16, 5))\n","\n","for j in range(6):\n","    axs[0, j].imshow(example_x[j, :, :].squeeze(), cmap='binary')\n","    axs[1, j].imshow(example_reconstruction[j, :, :], cmap='binary')\n","    axs[0, j].axis('off')\n","    axs[1, j].axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mPjoC1Gdcif","colab_type":"code","colab":{}},"source":["# Generate an example - sample a z value, then sample a reconstruction from p(x|z)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FxCMz-6dcih","colab_type":"code","colab":{}},"source":["# Display generated_x\n","\n","f, axs = plt.subplots(1, 6, figsize=(16, 5))\n","for j in range(6):\n","    axs[j].imshow(generated_x[j, :, :].numpy().squeeze(), cmap='binary')\n","    axs[j].axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5q8rrbQLdcik","colab_type":"code","colab":{}},"source":["# -ELBO estimate using an estimate of the KL divergence\n","\n","def loss(x, encoding_dist, sampled_decoding_dist,\n","         prior, sampled_z):\n","    recon_loss = -sampled_decoding_dist.log_prob(x)\n","    kl_approx = (encoding_dist.log_prob(sampled_z)\n","                    - prior.log_prob(sampled_z))\n","    return tf.reduce_sum(kl_approx + recon_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8bTToe6kdcil","colab_type":"text"},"source":["***\n","<a class=\"anchor\" id=\"tutorial4\"></a>\n","# KL divergence layers"]},{"cell_type":"code","metadata":{"id":"ICmU7PyKdcim","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten, Reshape\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-ppfASedcio","colab_type":"code","colab":{}},"source":["# Import Fashion MNIST\n","\n","(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n","x_train = x_train.astype('float32')/256. + 0.5/256\n","x_test = x_test.astype('float32')/256. + 0.5/256\n","example_x = x_test[:16]\n","\n","batch_size = 32\n","x_train = tf.data.Dataset.from_tensor_slices((x_train, x_train)).batch(batch_size)\n","x_test = tf.data.Dataset.from_tensor_slices((x_test, x_test)).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MkWpVW7dcir","colab_type":"code","colab":{}},"source":["# Define latent_size and the prior, p(z)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hDbG0SFdcit","colab_type":"code","colab":{}},"source":["# Define the encoding distribution using a tfpl.KLDivergenceAddLoss layer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32GEejTadciv","colab_type":"code","colab":{}},"source":["# See how `KLDivergenceAddLoss` affects `encoder.losses`\n","# encoder.losses before the network has received any inputs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVJ0kpgAdciy","colab_type":"code","colab":{}},"source":["# Pass a batch of images through the encoder\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OqVqaE-Vdci0","colab_type":"code","colab":{}},"source":["# See how encoder.losses has changed\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2l3nqJLdci2","colab_type":"code","colab":{}},"source":["# Re-specify the encoder using `weight` and `test_points_fn`\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9nXcFZ1dci4","colab_type":"code","colab":{}},"source":["# Replacing `KLDivergenceAddLoss`  with `KLDivergenceRegularizer` in the previous (probabilistic) layer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pl9o40sbdci6","colab_type":"code","colab":{}},"source":["# Specify the decoder, p(x|z)\n","\n","decoder = Sequential([\n","    Dense(16, activation='sigmoid', input_shape=(latent_size,)),\n","    Dense(32, activation='sigmoid'),\n","    Dense(64, activation='sigmoid'),\n","    Dense(2*event_shape[0]*event_shape[1], activation='exponential'),\n","    Reshape((event_shape[0], event_shape[1], 2)),\n","    tfpl.DistributionLambda(\n","        lambda t: tfd.Independent(\n","            tfd.Beta(concentration1=t[..., 0],\n","                     concentration0=t[..., 1])\n","        )\n","    )\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OhBTRmUdci9","colab_type":"code","colab":{}},"source":["# Connect the encoder and decoder to form the VAE\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfxEVT1kdcjA","colab_type":"code","colab":{}},"source":["# Define a loss that only estimates the expected reconstruction error,\n","# -E_{Z ~ q(z|x)}[log p(x|Z)]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4GZsFl1dcjB","colab_type":"code","colab":{}},"source":["# Compile and fit the model\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNKCeC7SdcjF","colab_type":"code","colab":{}},"source":["# Generate an example reconstruction\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4E-kRl35dcjG","colab_type":"code","colab":{}},"source":["# Plot the example reconstructions\n","\n","f, axs = plt.subplots(2, 6, figsize=(16, 5))\n","\n","for j in range(6):\n","    axs[0, j].imshow(example_x[j, :, :].squeeze(), cmap='binary')\n","    axs[1, j].imshow(example_reconstruction[j, :, :], cmap='binary')\n","    axs[0, j].axis('off')\n","    axs[1, j].axis('off')"],"execution_count":null,"outputs":[]}]}