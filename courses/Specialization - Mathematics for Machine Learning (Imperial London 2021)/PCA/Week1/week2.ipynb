{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 Assessment: Distances and Angles between Images\n",
    "\n",
    "In this week, we are going to compute the distances and angles between images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "1. Write program to compute \"distances\" between images.\n",
    "2. Write program to compute \"angles\" between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that the distance defined by the dot product is \n",
    "$$ d(x,y) = \\lVert x - y \\rVert, $$\n",
    "and the angle defined by the dot product is \n",
    "$$ x^T y = \\lVert x \\rVert \\lVert y \\rVert cos \\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def distance(x0, x1):\n",
    "    \"\"\"Compute distance between two vectors x0, x1 using the dot product\"\"\"\n",
    "    x0 = np.array(x0, dtype=np.float).ravel()\n",
    "    x1 = np.array(x1, dtype=np.float).ravel()\n",
    "    distance = -1\n",
    "    return distance\n",
    "\n",
    "\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def angle(x0, x1):\n",
    "    \"\"\"Compute the angle between two vectors x0, x1 using the dot product\"\"\"\n",
    "    angle = -1\n",
    "    return angle\n",
    "\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def pairwise_distance_matrix(X, Y):\n",
    "    \"\"\"Compute the pairwise distance between rows of X and rows of Y\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    X: ndarray of size (N, D)\n",
    "    Y: ndarray of size (M, D)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    D: matrix of shape (N, M), each entry D[i,j] is the distance between\n",
    "    X[i] and Y[j] using the dot product.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    M, _ = Y.shape\n",
    "    distance_matrix = np.zeros((N, M))\n",
    "    distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `pairwise_distance_matrix`, you may be tempting to iterate through\n",
    "rows of X and Y and fill in the distance matrix, but that is slow! Can you\n",
    "think of some way to vectorize your computation (i.e. make it faster by using numpy/scipy operations only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_vector(v, w):\n",
    "    \"\"\"Plot two vectors `v` and `w` of dimension 2\"\"\"\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    ax = fig.gca()\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.grid()\n",
    "    ax.arrow(0, 0, v[0], v[1], head_width=0.05, head_length=0.1, \n",
    "             length_includes_head=True, linewidth=2, color='r');\n",
    "    ax.arrow(0, 0, w[0], w[1], head_width=0.05, head_length=0.1, \n",
    "             length_includes_head=True, linewidth=2, color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some sanity checks, you may want to have more interesting test cases to test your implementation\n",
    "a = np.array([1,0])\n",
    "b = np.array([0,1])\n",
    "np.testing.assert_almost_equal(distance(a, b), np.sqrt(2))\n",
    "assert((angle(a,b) / (np.pi * 2) * 360.) == 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_vector(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from ipywidgets import interact\n",
    "MNIST = fetch_mldata('MNIST original', data_home='./MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(MNIST.data[MNIST.target==0].reshape(-1, 28, 28)[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we have the following questions:\n",
    "\n",
    "1. What does it mean for two digits in the MNIST dataset to be _different_ by our distance function? \n",
    "2. Furthermore, how are different classes of digits different for MNIST digits? Let's find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first question, we can see just how the distance between digits compare among all distances for \n",
    "the first 500 digits;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances = []\n",
    "for i in range(len(MNIST.data[:500])):\n",
    "    for j in range(len(MNIST.data[:500])):\n",
    "        distances.append(distance(MNIST.data[i], MNIST.data[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact(first=(0, 499), second=(0, 499), continuous_update=False)\n",
    "def show_img(first, second):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    f = MNIST.data[first].reshape(28, 28)\n",
    "    s = MNIST.data[second].reshape(28, 28)\n",
    "    \n",
    "    ax0 = plt.subplot2grid((2, 2), (0, 0))\n",
    "    ax1 = plt.subplot2grid((2, 2), (1, 0))\n",
    "    ax2 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n",
    "    \n",
    "    #plt.imshow(np.hstack([f,s]), cmap='gray')\n",
    "    ax0.imshow(f, cmap='gray')\n",
    "    ax1.imshow(s, cmap='gray')\n",
    "    ax2.hist(np.array(distances), bins=50)\n",
    "    d = distance(f, s)\n",
    "    ax2.axvline(x=d, ymin=0, ymax=40000, color='C4', linewidth=4)\n",
    "    ax2.text(0, 46000, \"Distance is {:.2f}\".format(d), size=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def most_similar_image():\n",
    "    \"\"\"Find the index of the digit, among all MNIST digits (excluding the first),\n",
    "       that is the closest to the first image in the dataset, your answer should be a single integer\n",
    "    \"\"\"\n",
    "    most_similar_index = -1\n",
    "    return most_similar_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second question, we can compute a `mean` image for each class of image, i.e. we compute mean image for digits of `1`, `2`, `3`... `9`, then we compute pairwise distance between them. We can organize the pairwise distances in a 2D plot, which would allow us to visualize the dissimilarity between images of different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the mean for digits of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = {}\n",
    "for n in np.unique(MNIST.target).astype(np.int):\n",
    "    means[n] = np.mean(MNIST.data[MNIST.target==n], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of classes, we compute the pairwise distance and \n",
    "store them into MD (mean distances). We store the angles between the mean digits in AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MD = np.zeros((10, 10))\n",
    "AG = np.zeros((10, 10))\n",
    "for i in means.keys():\n",
    "    for j in means.keys():\n",
    "        MD[i, j] = distance(means[i], means[j])\n",
    "        AG[i, j] = -1 # Complete this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the distances! Here we put the pairwise distances. The colorbar\n",
    "shows how the distances map to color intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "grid = ax.imshow(MD, interpolation='nearest')\n",
    "ax.set(title='Distances between different classes of digits',\n",
    "       xticks=range(10), \n",
    "       xlabel='class of digits',\n",
    "       ylabel='class of digits',\n",
    "       yticks=range(10))\n",
    "fig.colorbar(grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "grid = ax.imshow(AG, interpolation='nearest')\n",
    "ax.set(title='Angles between different classes of digits',\n",
    "       xticks=range(10), \n",
    "       xlabel='class of digits',\n",
    "       ylabel='class of digits',\n",
    "       yticks=range(10))\n",
    "fig.colorbar(grid)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K Nearest Neighbors (Optional)\n",
    "\n",
    "\n",
    "In this section, we will explore the [KNN classification algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\n",
    "A classification algorithm takes input some data and use the data to \n",
    "determine which class (category) this piece of data belongs to.\n",
    "\n",
    "![flower](https://archive.ics.uci.edu/ml/assets/MLimages/Large53.jpg)\n",
    "\n",
    "As a motivating example, consider the [iris flower dataset](https://archive.ics.uci.edu/ml/datasets/iris). The dataset consists\n",
    "of 150 data points where each data point is a feature vector $x \\in \\mathbb{R}^4$ describing the attribute of a flower in the dataset, the four dimensions represent \n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "\n",
    "and the corresponding target $y \\in \\mathbb{R}$ describes the class of the flower. There are 3 classes of flowers in this dataset.\n",
    "\n",
    "1. Iris Setosa\n",
    "2. Iris Versicolour \n",
    "3. Iris Virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "iris = datasets.load_iris()\n",
    "print('x shape is {}'.format(iris.data.shape))\n",
    "print('y shape is {}'.format(iris.target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simplicity of the exercise, we will only use the first 2 dimensions of $x$ to classify the flowers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.data[:, :2] # use first two features for simplicity\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the dataset. Here, we plot the features $x_0$,\n",
    "$x_1$ of the flowers. The colors encode the classes of the flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold  = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y,\n",
    "           cmap=cmap_bold, edgecolor='k', \n",
    "           s=20);\n",
    "ax.legend()\n",
    "ax.set(xlabel='$x_0$', ylabel='$x_1$', title='Iris flowers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the KNN algorithm is quite simple: assuming we have some training set of flowers $(X, y)$ \n",
    "which we know the features and which classes the flowers in the training set belong to. Now suppose we are predicting the class for an unseen flower $x_test$. To do this. We\n",
    "\n",
    "1. Compute its distance with all flowers in the training set.\n",
    "2. Find the $K$ \"closest\" flowers in the training set by our distance meature.\n",
    "3. From the $K$ flowers, find the \"major class\" $y_{test}$ these $K$ flowers belong to. You can think of this as the \n",
    "$K$ flowers \"voting\" for a class that $x_test$ should belong to.\n",
    "4. Predict the class of the unseen flower as $y_{test}$.\n",
    "\n",
    "If you are intereted in learning more about the KNN algorithm, also check out this [link](http://cs231n.github.io/classification/#knn) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KNN(k, X, y, Xtest):\n",
    "    \"\"\"K nearest neighbors\n",
    "    Arguments\n",
    "    ---------\n",
    "    k: int using k nearest neighbors.\n",
    "    X: the training set\n",
    "    y: the classes\n",
    "    Xtest: the test set which we want to predict the classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ypred: predicted classes for Xtest\n",
    "    \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    M, _ = Xtest.shape\n",
    "    num_classes = len(np.unique(y))\n",
    "    \n",
    "    # 1. Compute distance with all flowers\n",
    "    distance = np.zeros((N, M)) # EDIT THIS to use pairwise distances\n",
    "\n",
    "    # 2. Find indices for the k closest flowers\n",
    "    idx = np.argsort(distance.T, axis=1)[:, :K]\n",
    "    \n",
    "    # 3. Vote for the major class\n",
    "    ypred = np.zeros((M, num_classes))\n",
    "\n",
    "    for m in range(M):\n",
    "        klasses = y[idx[m]]    \n",
    "        for k in np.unique(klasses):\n",
    "            ypred[m, k] = len(klasses[klasses == k]) / K\n",
    "\n",
    "    return np.argmax(ypred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at how KNN behaves for the iris dataset.\n",
    "\n",
    "In the cell below, we select a grid of points and use our KNN algorithm to predict the classes \n",
    "for each of the point on the grid. the color of the mesh shows the prediction of KNN at \n",
    "a particular point on the grid. \n",
    "\n",
    "Pay close attension to the boundaries (in fact, they are called\n",
    "decision boundaries since points that lie on the boundaries can be predicted to either of the two\n",
    "classes on the left and right of the boundaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "step = 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, step),\n",
    "                     np.arange(y_min, y_max, step))\n",
    "\n",
    "ypred = []\n",
    "data = np.array([xx.ravel(), yy.ravel()]).T\n",
    "ypred = KNN(K, X, y, data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "ax.pcolormesh(xx, yy, ypred.reshape(xx.shape), cmap=cmap_light)\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor='k', s=20);\n",
    "ax.set(xlabel='$x_0$', ylabel='$x_1$', title='KNN decision boundary (K={})'.format(K));"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "mathematics-machine-learning-pca",
   "graded_item_id": "kGOjp",
   "launcher_item_id": "Myc4L"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
