{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Monte Carlo Methods\n",
    "\n",
    "### References\n",
    "\n",
    "[Beginning Bayesian Statistics](https://pubs.er.usgs.gov/publication/70204463)\n",
    "\n",
    "[Hamiltonian Monte Carlo in Python](https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/)\n",
    "\n",
    "[Betancourt HMC - Best introduction to HMC](https://www.youtube.com/watch?v=VnNdhsm0rJQ)\n",
    "\n",
    "[NUTS paper](http://arxiv.org/abs/1111.4246)\n",
    "\n",
    "[HMC Tuning by Colin Caroll](https://colcarroll.github.io/hmc_tuning_talk/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Monte Carlo simulation?\n",
    "\n",
    "Monte Carlo simulations refer to any set of simulations that samples a lot of values from some distribution to estimate this distribution. \n",
    "\n",
    "### Why do we need it?\n",
    "\n",
    "Earlier we saw the use of conjugate solutions to compute the posterior distribution directly. A lot of the times closed form solutions are not available, and we have to resort to discretization and quadrature rules to evaluate these posterior distributions. There are times when even this isn't feasible. In these situations, we resort to sampling from the posterior to estimate the distribution.\n",
    "\n",
    "We look at three specific algorithms here:\n",
    "\n",
    "1. Metropolis \n",
    "2. Metropolis-Hastings\n",
    "3. Gibbs Sampling\n",
    "\n",
    "A fourth one, the Hamiltonian Monte Carlo algorithm based on Hamiltonian mechanics is briefly illustrated here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks\n",
    "\n",
    "#### Markov Chains\n",
    "\n",
    "[Intuitive explanation of Markov Chains](https://brilliant.org/wiki/markov-chains/)\n",
    "\n",
    "* A Markov chain is a stochastic system that transitions from one state to another based on a probability. For e.g. the position and velocity of a moving car could be the variables at times \\\\(t_0, t_1..t_n\\\\) at states given by \\\\(S_0, S_1...S_n\\\\). This is illustrated in the figure below. \n",
    "\n",
    "* The probability of moving to a new state is independent of how one reached the current state, i.e. the probability of moving to a new state only depends on the current state and the transition probability to the new state. To rephrase, one transitions from state \\\\(S_{t-1}\\\\) to state \\\\(S_{t}\\\\) based upon the transition probability given by the conditional probability\\\\(P(S_t | S_{t-1})\\\\). \n",
    "\n",
    "* We can also make observations at each state indicated by the observation nodes. In the example, the recordings using a radar (possibly noisy) at each state would be the observations.\n",
    "\n",
    "* An absorbing state is one where the probability of staying at that state is 1, i.e. there is zero chance of leaving that state. A recurring state is one where there is a finite probability of returning to that state.\n",
    "\n",
    "* If the state variables are discrete the model is called a Hidden Markov Model. \n",
    "\n",
    "![Markov Model](https://courseraimages.s3-us-west-2.amazonaws.com/MM.png)\n",
    "\n",
    "\n",
    "#### Stationary distributions\n",
    "\n",
    "*Reference* [Youtube](https://www.youtube.com/watch?v=aIdTGKjQWjA)\n",
    "\n",
    "Stationary distributions simply mean that the probability of the distribution at a time t+1 is the same as that at time t. Using the terminology from the above section, the transition probability is the same for all t. Ergodic distributions are therefore stationary.\n",
    "\n",
    "#### Ergodicity\n",
    "\n",
    "Ergodicity is a critical property for a Markov Chain which combine the properties of a state being recurrent and aperiodic. Recurrent implies that, in a transition, a given state will return to itself. An aperiodic state returns to itself in a number of steps 1,2,3.... $\\infty$. The implications of ergodicity are \n",
    "\n",
    "1. If we sample a space long enough we will cover almost every point in that space (theoretically). \n",
    "\n",
    "2. If we obtain a statistic from a sequence such as the mean, this statistic should be the same if we recompute it using a different sequence drawn from the same set of events. The implication here is that there is only one distribution unlike a non-stationary distribution which has an infinite set of PDFs. \n",
    "\n",
    "\n",
    "### Why does this work?\n",
    "\n",
    "Suppose there is a system with three states given by A,B and C and there is a person who can transition from A \\\\(\\longleftrightarrow\\\\) B \\\\(\\longleftrightarrow\\\\) C with transition probabilities given by \\\\(p_{AB}\\\\), \\\\(p_{BC}\\\\) for transitioning to the right and \\\\(p_{BA}\\\\), \\\\(p_{CB}\\\\) for transitioning to the left. *The gist of the MCMC process is that the ratio of the transition probabilities of the two states equals the relative probabilities of the two states in the distribution*. \n",
    "\n",
    "Let us consider state B and the transition to state C. \n",
    "\n",
    "* The probability of moving to state C is given by the product of the probability of choosing C (since B can move to either A or C) and the probability of accepting the move to C given by \\\\(min(\\dfrac{P_C}{P_B},1)\\\\). \n",
    "\n",
    "* The last part of accepting the proposed move is the interesting part, we decide to move depending on the ratio of the probability densities \\\\(P_C\\\\) and \\\\(P_B\\\\). The transition probability from B to C can therefore be written as \n",
    "\n",
    "$$p_{BC} = 0.5 \\cdot min(\\dfrac{P_C}{P_B},1)$$\n",
    "\n",
    "* The transition probability from C to B can be similarly written as \n",
    "\n",
    "$$ p_{CB} = 0.5 \\cdot min(\\dfrac{P_B}{P_C},1)$$\n",
    "\n",
    "* If we take the ratio of these probabilities\n",
    "\n",
    "$$\\dfrac{p_{BC}}{p_{CB}} = \\dfrac{0.5 \\cdot min(\\dfrac{P_C}{P_B},1)}{0.5 \\cdot min(\\dfrac{P_B}{P_C},1)}$$\n",
    "\n",
    "* If $P_C$ > $P_B$ we get \n",
    "\n",
    "$$\\dfrac{p_{BC}}{p_{CB}} = \\dfrac{P_C}{P_B}$$\n",
    "\n",
    "* If \\\\(P_B > P_C\\\\), we still obtain the same term as a result of the min() function\n",
    "\n",
    "$$\\dfrac{p_{BC}}{p_{CB}} = \\dfrac{P_C}{P_B}$$\n",
    "\n",
    "Hence, the ratio of the transition probabilities between states can be seen as the ratio of their probability densities. In other words, if we run this experiment long enough where our volunteer has agreed to move between the states based on the transition probabilities, the adjacent positions are visited a number of times that is proportional to their relative probability densities in the target distribution. We can extend this argument for all points to suggest that if we run the experiment long enough, we can build the target distribution.\n",
    "\n",
    "\n",
    "#### Proposal distribution\n",
    "An easy to sample distribution such as a Gaussian distribution $q(x)$ such that \n",
    "\n",
    "$$q(x_{i+1} | x_{i}) \\sim N(\\mu, \\sigma)$$\n",
    "\n",
    "### The Bayesian Inference Process\n",
    "\n",
    "1. Obtain the data and inspect it for a high-level understanding of the distribution of the data and the outliers\n",
    "2. Define a reasonable prior for the data based on (1) and your understanding of the problem\n",
    "3. Define a likelihood distribution for the data and obtain the likelihood of the data given this likelihood distribution\n",
    "4. Obtain the posterior distribution using (2) and (3) by applying the Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Metropolis Algorithm\n",
    "\n",
    "#### Problem Statement\n",
    "\n",
    "We start off by modeling a discrete number of events using a Poisson distribution shown below. \n",
    "\n",
    "$$f(x) = e^{-\\mu} \\mu^x / x!$$\n",
    "\n",
    "The mean rate is represented by Î¼ and x is positive integer that represents the number of events that can happen. If you recall from the discussion of the binomial distribution, that can also be used to model the probability of the number of successes out of 'n' trials. The Poisson distribution is a special case of this binomial distribution and is used when the trials far exceed the number of successes.\n",
    "\n",
    "If our observed data has a Poisson likelihood distribution, using a Gamma prior for \\\\(\\mu\\\\) results in a Gamma posterior distribution. \n",
    "\n",
    "#### Outline of the Metropolis algorithm\n",
    "*What do we want to compute?*\n",
    "\n",
    "To estimate a distribution of a parameter $\\mu$\n",
    "\n",
    "*What do we have available?*\n",
    "\n",
    "Observed data\n",
    "\n",
    "*How do we do it?*\n",
    "\n",
    "1. Start with a parameter sample $\\mu_{current}$ that is drawn from a distribution\n",
    "2. Draw a second parameter sample $\\mu_{proposed}$ from a proposal distribution\n",
    "3. Compute the likelihood of the data for both the parameters\n",
    "4. Compute the prior probability density of both the parameters\n",
    "5. Compute the posterior probability density of both parameters by multiplying the prior and the likelihood from (3) and (4)\n",
    "6. Select one parameter from the posterior probability density computed above using a rule and save the selected one as $\\mu_{current}$ \n",
    "7. Repeat steps (2) to (7) till a large number of parameters have been drawn (usually around 5000, but this really depends on the problem)\n",
    "8. Compute the distribution of the parameter $\\mu$ by plotting a histogram of the saved sampled parameter $\\mu_{current}$ in step (6)\n",
    "\n",
    "#### The details\n",
    "\n",
    "##### Notes\n",
    "\n",
    "* I am leaving the hyperparameters and data undefined to make it easier to track and follow the sampled parameter values in these equations.\n",
    "\n",
    "* We are assuming that we have a single data point 'x' to keep this easier to understand. This implies that the number of data points given by 'n' = 1.\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "1. Propose a single plausible value for our parameter $\\mu$. This is $\\mu_{current}$, and it is also called the current value. Let us assume that this is 7.5 for now.\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "2. Compute the prior probability density of getting \\\\(\\mu = 7.5\\\\). We stated earlier in our example that we have selected a Gamma prior distribution for our parameter $\\mu$.\n",
    "\n",
    "   $$Gamma(\\mu=7.5 |\\alpha, \\beta) = \\beta^{\\alpha} \\mu^{\\alpha - 1} e^{-\\beta \\mu} / \\gamma(\\alpha) = \\beta^{\\alpha} 7.5^{\\alpha - 1} e^{-\\beta 7.5} / \\gamma(\\alpha)$$\n",
    "\n",
    "    \n",
    "3. Compute the likelihood of the data 'x', given the parameter value of 7.5.  The likelihood distribution was a Poisson distribution in our example and is given by\n",
    "\n",
    "   $$Poisson(x | \\mu=7.5) = e^{-\\mu} \\mu^x / x! = e^{-7.5} 7.5^x / x!$$\n",
    "\n",
    "\n",
    "4. Compute the posterior density from (2) and (3), we skip the denominator here since we are only going to make comparisons and the denominator is a constant. \n",
    "\n",
    "   $$Posterior\\; density \\propto Prior \\cdot Likelihood $$\n",
    "   \n",
    "   The Gamma distribution is parameterized by the shape parameter $\\alpha$ and the rate parameter $\\beta$. If the prior distribution for the mean parameter $\\mu$ is given by a Gamma distribution parameterized by $\\alpha_{prior}$ and $\\beta_{prior}$ and given 'n' observations (In our example, n = 1)\n",
    "\n",
    "   $$\\alpha_{posterior} = \\alpha_{prior} + \\sum_{i=0}^n x_i$$\n",
    "\n",
    "   $$\\beta_{posterior} = \\beta_{prior} + n$$\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "5. Propose a second value for $\\mu$, called $\\mu_{proposed}$, which is drawn from a distribution called a proposal distribution centered on $\\mu_{current}$. This value is called the proposed value. For the Metropolis algorithm, it has to be a symmetrical distribution. We will use a normal distribution for this example and set the mean of this proposal distribution to be the current value of $\\mu$ or $\\mu_{current}$. The standard deviation of the proposal distribution is a hyperparameter called the tuning parameter. Let us assume that we draw a value of 8.5 for $\\mu_{proposed}$.\n",
    "\n",
    "6. Compute the prior, likelihood and the posterior for this proposed value of $\\mu$ or $\\mu_{proposed}$ as we did in step (2), (3) and (4).\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "7. Select one value from the current and the proposed value with the following two steps (this step is where the Metropolis algorithm differs from the Metropolis-Hastings algorithm)  \n",
    "\n",
    "   a. Compute the probability of moving to the proposed value as\n",
    "   \n",
    "      $$p_{move} = min( \\dfrac{P(\\mu_{proposed} | data)}{P(\\mu_{current} | data)}, 1)$$\n",
    "\n",
    "      Here $p_{move}$ is the minimum of the values given by the ratio of the posterior probabilities of \\\\(\\mu_{proposed}\\\\) and \\\\(\\mu_{current}\\\\), and the number 1. This caps the probability $p_{move}$ at 1 if the ratio happens to be greater than 1. $P_{move}$ is also referred to as the transition kernel.\n",
    "\n",
    "   b. Draw a sample from a uniform distribution U(0,1). If $p_{move}$ from (a) above is greater than this number drawn from the uniform distribution, we accept the proposed value $\\mu_{proposed}$. What this means is that if the posterior density of the proposed parameter value is greater than the posterior density of the current parameter value, then we move to the proposed value otherwise we probabilistically accept the proposed value based on the value of $p_{move}$ and the randomly drawn value from the uniform distribution.\n",
    "\n",
    "8. If we moved to the proposed value, save the current value, i.e. $\\mu_{current}$, to an array and then update the current value with the proposed value. In the next iteration, the current value $\\mu^{i+1}_{current}$ will be this accepted proposed value $\\mu^{i}_{proposed}$.\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "9. Repeat steps (2) to (8) thousands of times and plot the histogram of the accepted values, i.e. the array of current values $\\mu_{current}$.\n",
    "   \n",
    "\n",
    "#### Traceplot \n",
    "\n",
    "This is a plot of the sequence of accepted values from the proposed values, plotted over each draw. If a proposed value was not accepted, you see the same value repeated again. If you notice a straight line, this is an indication that several proposed values are being rejected. This is a sign that something is askew with the distribution or sampling process.\n",
    "\n",
    "\n",
    "#### Building the Inferred Distribution\n",
    "\n",
    "Use the current values that we obtain at each step and build a frequency distribution (histogram) from it.\n",
    "\n",
    "#### Representing the Inferred Distribution\n",
    "\n",
    "* Compute the mean values of the saved parameters\n",
    "* Compute the standard deviation and variance of the saved parameters\n",
    "* Compute the minimum and maximum values of the saved parameters\n",
    "* Compute the quantiles of the saved parameters\n",
    "* If required, express it as the parameters of a canonical distribution if it is known that the inferred distribution will be of a certain form.\n",
    "\n",
    "\n",
    "#### Notes about the Metropolis Algorithm\n",
    "\n",
    "* The proposal distribution has to be symmetric, this condition is relaxed in the Metropolis-Hastings algorithm. A normal distribution is commonly used as a proposal distribution in the Metropolis algorithm.\n",
    "\n",
    "* The choice of a prior distribution influences the performance of this algorithm.\n",
    "\n",
    "* The tuning parameter is a hyperparameter, i.e. the standard deviation of the proposal distribution is essential to tune this proposal distribution. This needs to be tuned such that the acceptance probability is a certain value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Code for Walkthrough of the Metropolis Algorithm\n",
    "\n",
    "Using the example above, we are going to look at code to run the Metropolis algorithm for 1000 iterations. This will simulate the inference process. Around 5000 iterations will get you closer to the true posterior distribution.\n",
    "\n",
    "#### Problem Statement\n",
    "\n",
    "This is a trivial problem and is only designed to help illustrate the workings of the Metropolis algorithm with a single data point. \n",
    "\n",
    "We observe about 9 power outages in a year in Charlottesville, VA and we know that the number of power outages per year can be modeled by a Poisson distribution with parameter \\\\(\\lambda\\\\). The parameter \\\\(\\lambda\\\\) is drawn from a Gamma prior distribution that has parameters $\\alpha$ = 7 and $\\beta$ = 1. If this is the only observation we have available, in a frequentist world, we would conclude that the rate parameter is 9, but in a Bayesian landscape we have some knowledge about this rate parameter based on what we may have observed in the past. This past knowledge gets incorporated into the prior and tempers our belief so we are not making an assumption based only on what we just saw. In a way, you can consider this a way of online learning since the posterior of the rate parameter from the past data can be incorporated as a prior in our current inference process, resulting in continuous refinement of our posterior based on new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 11.,  66., 139., 248., 176., 137.,  93.,  75.,  45.,  10.]),\n",
       " array([ 3.21653055,  4.2307926 ,  5.24505464,  6.25931668,  7.27357873,\n",
       "         8.28784077,  9.30210282, 10.31636486, 11.33062691, 12.34488895,\n",
       "        13.359151  ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 375.2875 248.518125 \n",
       "L 375.2875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 33.2875 224.64 \n",
       "L 368.0875 224.64 \n",
       "L 368.0875 7.2 \n",
       "L 33.2875 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 48.505682 224.64 \n",
       "L 78.942045 224.64 \n",
       "L 78.942045 215.454747 \n",
       "L 48.505682 215.454747 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 78.942045 224.64 \n",
       "L 109.378409 224.64 \n",
       "L 109.378409 169.528479 \n",
       "L 78.942045 169.528479 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 109.378409 224.64 \n",
       "L 139.814773 224.64 \n",
       "L 139.814773 108.571797 \n",
       "L 109.378409 108.571797 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 139.814773 224.64 \n",
       "L 170.251136 224.64 \n",
       "L 170.251136 17.554286 \n",
       "L 139.814773 17.554286 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 170.251136 224.64 \n",
       "L 200.6875 224.64 \n",
       "L 200.6875 77.675945 \n",
       "L 170.251136 77.675945 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 200.6875 224.64 \n",
       "L 231.123864 224.64 \n",
       "L 231.123864 110.241843 \n",
       "L 200.6875 110.241843 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 231.123864 224.64 \n",
       "L 261.560227 224.64 \n",
       "L 261.560227 146.982857 \n",
       "L 231.123864 146.982857 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 261.560227 224.64 \n",
       "L 291.996591 224.64 \n",
       "L 291.996591 162.013272 \n",
       "L 261.560227 162.013272 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 291.996591 224.64 \n",
       "L 322.432955 224.64 \n",
       "L 322.432955 187.063963 \n",
       "L 291.996591 187.063963 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path clip-path=\"url(#p6d424357c2)\" d=\"M 322.432955 224.64 \n",
       "L 352.869318 224.64 \n",
       "L 352.869318 216.28977 \n",
       "L 322.432955 216.28977 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mec30c8d202\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"72.016333\" xlink:href=\"#mec30c8d202\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 4 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(68.835083 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"132.033098\" xlink:href=\"#mec30c8d202\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 6 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(128.851848 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.049864\" xlink:href=\"#mec30c8d202\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 8 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(188.868614 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.066629\" xlink:href=\"#mec30c8d202\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(245.704129 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"312.083395\" xlink:href=\"#mec30c8d202\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 12 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(305.720895 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m9882bdbb15\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m9882bdbb15\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(19.925 228.439219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m9882bdbb15\" y=\"182.888848\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 50 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(13.5625 186.688067)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m9882bdbb15\" y=\"141.137696\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(7.2 144.936915)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m9882bdbb15\" y=\"99.386544\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(7.2 103.185763)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m9882bdbb15\" y=\"57.635392\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(7.2 61.43461)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m9882bdbb15\" y=\"15.88424\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(7.2 19.683458)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 33.2875 224.64 \n",
       "L 33.2875 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 368.0875 224.64 \n",
       "L 368.0875 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 33.2875 224.64 \n",
       "L 368.0875 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 33.2875 7.2 \n",
       "L 368.0875 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6d424357c2\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gamma, factorial\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "# Prior alpha = 7, beta = 1\n",
    "# Start with a value of lambda given by 8.0 and compute the prior probability density of observing this value\n",
    "\n",
    "def prior_prob_density(lam, alpha, beta):\n",
    "     return(beta**(alpha) * lam**(alpha - 1) * np.exp(-beta*lam) / gamma(alpha))\n",
    "\n",
    "def likelihood_density(data, lam):\n",
    "    return(lam**(data) * np.exp(-lam)/ factorial(data))\n",
    "\n",
    "# Starting value of lambda\n",
    "lambda_current = 8.0\n",
    "# Prior parameters alpha and beta\n",
    "alpha = 7.0\n",
    "beta = 1.0\n",
    "# Observed data of 9 outages \n",
    "data_val = 9\n",
    "\n",
    "lambda_array = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # Current value \n",
    "    prior = prior_prob_density(lam=lambda_current, alpha=alpha, beta=beta)\n",
    "    likelihood = likelihood_density(data=data_val, lam=lambda_current)\n",
    "    posterior_current = likelihood * prior \n",
    "    \n",
    "    # Proposed value\n",
    "    lambda_proposed = np.random.normal(lambda_current, scale=0.5) # scale is our tuning parameter\n",
    "    prior = prior_prob_density(lam=lambda_proposed, alpha=alpha, beta=beta)\n",
    "    likelihood = likelihood_density(data=data_val, lam=lambda_proposed)\n",
    "    posterior_proposed = likelihood * prior\n",
    "    \n",
    "    # Compute the probability of move\n",
    "    ratio = posterior_proposed / posterior_current\n",
    "    p_move = min(ratio, 1)\n",
    "    random_draw = np.random.uniform(0,1)\n",
    "    if (random_draw < p_move):\n",
    "        lambda_current = lambda_proposed\n",
    "        \n",
    "    # Store the current value\n",
    "    lambda_array[i] = lambda_current\n",
    "\n",
    "plt.hist(lambda_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNGRADED EVALUATION (50 min)\n",
    "\n",
    "#### 1. Run the simulation for 10 iterations and print out the following as a table, with each trial being a row\n",
    "\n",
    "  a. the current parameter value and its hyperparameters\n",
    "\n",
    "  b. proposed parameter value and its hyperparameters\n",
    "\n",
    "  c. the posterior probabilities of both\n",
    "\n",
    "  d. the probability of move\n",
    "\n",
    "  e. the drawn random value\n",
    "\n",
    "  f. the decision as a binary value\n",
    "\n",
    "\n",
    "#### 2. Summarize the above distribution - Mean, Variance, Minimum and Maximum, Quartiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Metropolis-Hastings Algorithm\n",
    "\n",
    "#### Overview\n",
    "\n",
    "One of the limitations of the Metropolis algorithm was the requirement of a symmetric proposal distribution. The Metropolis-Hastings algorithm relaxes this requirement by providing a correction term if a non-symmetric proposal distribution is used. This correction is applied to $p_{move}$ and is of the form\n",
    "\n",
    "$p_{move} = min( \\dfrac{P(\\mu_{proposed} | data) \\cdot g(\\mu_{current} | \\mu_{proposed})}{P(\\mu_{current} | data) \\cdot g(\\mu_{proposed} | \\mu_{current})}, 1)$\n",
    "\n",
    "where the correction term is \n",
    "\n",
    "$\\dfrac{g(\\mu_{current} | \\mu_{proposed})}{g(\\mu_{proposed} | \\mu_{current})}$\n",
    "\n",
    "The term $g(\\mu_{current} | \\mu_{proposed})$ is the probability density of drawing $\\mu_{current}$ from a normal distribution centered around $\\mu_{proposed}$. The standard deviation for this normal distribution is the tuning parameter. For a symmetric proposal distribution such as a normal distribution the correction term would be 1 since the probability density of drawing $\\mu_{current}$ from a Gaussian centered at $\\mu_{proposal}$ only depends on the distance between $\\mu_{current}$ and $\\mu_{proposal}$ (standard deviation is a hyperparameter that is fixed). Similarly, the probability density of drawing $\\mu_{proposal}$ from a Gaussian centered around $\\mu_{current}$ depends only on the distance between these two values, which is the same as before. Hence the numerator and the denominator are the same which results in the correction factor being 1.\n",
    "\n",
    "#### Why do we need a correction term?\n",
    "\n",
    "The correction term exists to account for the lack of symmetry in a non-symmetric proposal distribution. The Metropolis algorithm is therefore a specific case of the Metropolis-Hastings algorithm. When distributions other than a Gaussian is used as a proposed distribution, one can center $\\mu_{current}$ and  $\\mu_{proposal}$ on the mean, median or mode of the distribution. It is also possible to draw samples from a fixed distribution, this technique is called the Independent Metropolis-Hastings sampling algorithm.\n",
    "\n",
    "#### What is the advantage of using a non-symmetric proposal distribution?\n",
    "\n",
    "If the parameter we are seeking is bounded in value, using a symmetric distribution can result in invalid draws. Also, since we are working in a Bayesian setting we want to take advantage of our prior knowledge of this parameter. If it is known that the parameter has a certain distribution, we should be able to incorporate this information into our sampling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNGRADED EVALUATION (2.5 hours)\n",
    "\n",
    "#### 1. Write Python code to modify the Metropolis algorithm from above to make it a Metropolis-Hastings algorithm \n",
    "\n",
    "#### 2. Using the example of power outages above \n",
    "\n",
    "1. Run a simulation for 1000 iterations and summarize the above distribution - Mean, Variance, Minimum and Maximum, Quartiles\n",
    "\n",
    "2. Run the simulation for 10 iterations and print out the following as a table, with each trial being a row\n",
    "\n",
    "   a. the current parameter value and its hyperparameters\n",
    "\n",
    "   b. proposed parameter value and its hyperparameters\n",
    "\n",
    "   c. the posterior probabilities of both\n",
    "\n",
    "   d. the correction factor\n",
    "\n",
    "   e. the probability of move\n",
    "\n",
    "   f. the drawn random value\n",
    "   \n",
    "   g. the decision as a binary value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### GRADED EVALUATION (30 mins)\n",
    "\n",
    "\n",
    "1. The ratio of the transition probabilities between states can be seen as the ratio of their probability densities\n",
    "\n",
    "    a. True\n",
    "\n",
    "    b. False\n",
    "\n",
    "\n",
    "2. Not using a reasonable prior can result in convergence issues when performing MCMC sampling\n",
    "\n",
    "    a. True\n",
    "\n",
    "    b. False \n",
    "\n",
    "\n",
    "3. Not using an appropriate proposal distribution during MCMC can result in inaccurate inferences about a parameter\n",
    "\n",
    "    a. True\n",
    "\n",
    "    b. False \n",
    "\n",
    "\n",
    "4. The Metropolis-Hastings algorithm differs from the Metropolis algorithm in terms of the correction term that is added to the Metropolis step\n",
    "\n",
    "    a. True\n",
    "\n",
    "    b. False \n",
    "\n",
    "\n",
    "5. The Metropolis algorithm is a specific case of the Metropolis-Hastings algorithm\n",
    "\n",
    "   a. True\n",
    "\n",
    "   b. False\n",
    "\n",
    "\n",
    "6. Why does a correction term exist in the Metropolis-Hastings algorithm?\n",
    "\n",
    "   a. To remove the errors introduced by the Metropolis algorithm\n",
    "\n",
    "   b. To correct for the lack of symmetry in a non-symmetric proposal distribution\n",
    "\n",
    "\n",
    "7. We use non-symmetric proposal distributions because\n",
    "\n",
    "   a. They are more fun to use!\n",
    "\n",
    "   b. To avoid invalid draws\n",
    "\n",
    "\n",
    "8. In the Metropolis algorithm, what is used as the tuning parameter if a Normal distribution is used as a proposal distribution?\n",
    "\n",
    "    a. Standard deviation\n",
    "    \n",
    "    b. Mean\n",
    "    \n",
    "\n",
    "9. Bayesian Inference can be seen as a type of online learning since\n",
    "\n",
    "    a. The inferred posterior can be used as the prior when new data arrives \n",
    "    \n",
    "    b. The prior can be reused again for new data\n",
    "    \n",
    "    \n",
    "10. If the traceplot displays a straight line, this is a sign that\n",
    "\n",
    "    a. The newly proposed values are being rejected\n",
    "    \n",
    "    b. The sampling has converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "\n",
    "In a Gibbs sampler, the proposal distribution matches the posterior conditional distribution and as a result the  proposals are always accepted (since there is no reason to reject unlike in the Metropolis algorithm where an arbitrary proposal distribution is used). This can be seen as a specific case of a Metropolis algorithm. One of the features of the Gibbs sampler is that it allows us to perform inference on more than one parameter at a time. This is done by drawing one parameter at a time conditional on the values of the other parameters. It iteratively works through the parameters using this process and continues till sufficient samples have been drawn for all parameters.\n",
    "\n",
    "Additionally, Gibbs Sampling can draw proposals from an asymmetric distribution. In the example below, we will be drawing from a Gamma distribution which is not symmetric. Not having a pre-determined proposal distribution is seen sometimes as an advantage. The disadvantage of this method, however, is that you are required to decompose the joint distribution into the conditional distributions in order to sample from them.  If the conjugate solutions are known, the Gibbs sampler can be faster than the Metropolis-Hastings algorithm.\n",
    "\n",
    "In the following example, we are going to to try the infer the parameters of a Normal distribution, i.e. the mean given by \\\\(\\mu\\\\) and the precision given by \\\\(\\tau\\\\). We use a Normal distribution here since we can use that to illustrate how Gibbs sampling can be used to estimate multiple parameters, i.e. \\\\(\\mu\\\\) and \\\\(\\tau\\\\), at the same time. \n",
    "\n",
    "\n",
    "#### The Problem Setup\n",
    "\n",
    "We are going to use Gibbs sampling to estimate the parameters of a model that is used to represent some phenomenon. For the sake of this exercise, let us say that this is a Normal distribution and we have one data point as our observation.\n",
    "\n",
    "Since we are using a Normal distribution, parameterized as $N(\\mu, \\tau)$ we will need the conjugate solution for computing our posterior from the priors. Here $\\mu$ is the mean and $\\tau$ is the precision of the Normal distribution. A word on notation as we proceed, using $\\tau$ as an example - the draws are denoted by numbered subscripts such as $\\tau_0$, $\\tau_1$ while the hyperparameters for the prior and posterior distributions are denoted as $\\tau_{prior}$ and $\\tau_{posterior}$ respectively. \n",
    "\n",
    "$$\\mu \\sim N(\\mu_{prior}, \\tau_{prior})$$\n",
    "\n",
    "Select $\\mu_{prior}$ to be 12 and $\\tau_{prior}$ to be 0.0625 which corresponds to a $\\sigma$ of 4.\n",
    "\n",
    "$$\\tau \\sim Gamma(\\alpha_{prior}, \\beta_{prior})$$ \n",
    "\n",
    "Select the shape parameter $\\alpha_{prior}$ to be 25 and the rate parameter $\\beta_{prior}$ to be 0.5. \n",
    "\n",
    "##### Conjugate Solution for Parameter \\\\(\\mu\\\\) with a Normal Prior\n",
    "\n",
    "$$\\mu_{posterior} = (\\tau_{prior} \\mu_{prior} + \\tau_0 \\sum_i x_i) / (\\tau_{prior} + n \\tau_0)$$\n",
    "\n",
    "$$\\tau_{posterior} = \\tau_{prior} + n * \\tau_0$$\n",
    "\n",
    "##### Conjugate Solution for  Parameter \\\\(\\tau\\\\) with a Gamma Prior\n",
    "\n",
    "$$\\alpha_{posterior} = \\alpha_{prior} + n/2$$\n",
    "\n",
    "$$\\beta_{posterior} = \\beta_{prior} + \\sum_i (x_i - \\mu_{1})^2 / 2$$\n",
    "\n",
    "\n",
    "#### Outline of the Algorithm\n",
    "\n",
    "1. Specify reasonable priors for the parameters $\\mu$ and $\\tau$.\n",
    "2. Choose one parameter from the two parameters above to start with, and assign an initial value. Let us assume that we start with $\\tau$ here and select a value of $\\tau_0$ from the Gamma prior distribution.\n",
    "3. Start our first trial. We want to obtain a sample for $\\mu$ from the posterior distribution of $\\mu$ given the value of $\\tau_0$. This is where we use our knowledge of the distribution and use a conjugate solution to obtain the posterior distribution of $\\mu$. Now we draw a sample $\\mu_1$ from this posterior distribution.\n",
    "4. We continue with trial 1 since we need to obtain a value for $\\tau_1$ conditional on the value of $\\mu_1$. Similar to step (3), we use the conjugate solution to obtain a posterior distribution of $\\tau$ given a value of $\\mu_1$. Draw a value of $\\tau_1$ from this distribution.\n",
    "5. We accept both values we have drawn in steps (3) and (4) and trial 1 is now complete. Note that unlike the Metropolis algorithm, we do not stochastically accept or reject the proposals, we accept all drawn values.\n",
    "6. Repeat steps (3) to (5) till we have a sufficient number of samples. This process of iteratively updating the parameters is loosely akin to coordinate ascent for optimization.\n",
    "\n",
    "\n",
    "#### The details\n",
    "\n",
    " \n",
    "**Please don't confuse $\\tau$, which is the parameter our Normal distribution, with $\\tau_0$ which is the hyperparameter of our mean $\\mu$.** \n",
    "\n",
    "We have one data point that we are going to use to illustrate how Gibbs sampling works.  Obviously, in a real example we will have multiple data points in which case we will have to compute the likelihood and posterior given all those data values. We will walk through the first two trials of Gibbs Sampling.                                            \n",
    "##### The Algorithm\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "a. With the priors set up, draw a value for $\\tau_0$ from the Gamma prior distribution. Let us assume that this is 40.123.\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "b. Start trial 1. Calculate the posterior distribution of $\\mu$ (Normal distribution) using the conjugate solution shown below. Here 'n' is the number of samples which happens to be 1 for our example.\n",
    "\n",
    "$$\\mu_{posterior} =(\\tau_{prior} \\mu_{prior} + \\tau_0 \\sum_i x_i) / (\\tau_{prior} + n \\tau_0)$$ \n",
    "$$= (0.0625 * 12 + 40.123 * 10.2) / (0.0625 + 1 * 40.123) = 10.2028$$\n",
    "$$\\tau_{posterior} = \\tau_{prior} + n * \\tau_0  = 0.0625 + 1 * 40.123 = 40.1855$$\n",
    "\n",
    "c. Draw a value for $\\mu_1$ from this computed posterior distribution for $\\mu$ from step (b). Let us assume that this value of $\\mu_1$ is <mark>10.5678</mark>.\n",
    "\n",
    "d. With the given value of $\\mu_1$, we now compute the posterior distribution of $\\tau$ using a conjugate solution for the Gamma distribution. \n",
    "\n",
    "$$\\alpha_{posterior} = \\alpha_{prior} + n/2 = 25 + 1/2 = 25.5$$\n",
    "\n",
    "$$\\beta_{posterior} = \\beta_{prior} + \\sum_i (x_i - \\mu_{1})^2 / 2 = 0.5 + (10.2 - 10.5678)^2 / 2 = 0.5676$$\n",
    "\n",
    "e. Draw a value for $\\tau_1$ from this posterior distribution computed in step (d). Let us assume that this value is <mark>45.678</mark>. Trial 1 is now complete.\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "f. Trial 2 will be similar to trial 1 except that we substitute the values for $\\tau_0$ and $\\mu_0$ with the updates $\\tau_1$ and $\\mu_1$ we obtained at the end of steps (c) and (e).\n",
    "\n",
    "g. Update the posterior for $\\mu$\n",
    "\n",
    "$$\\mu_{posterior} =(\\tau_{prior} \\mu_{prior} + \\tau_1 \\sum_i x_i) / (\\tau_{prior} + n \\tau_1) = (0.0625 * 12 + 45.678 * 10.2) / (0.0625 + 1 * 45.678) = 10.2025$$\n",
    "\n",
    "$$\\tau_{posterior} = \\tau_{prior} + n * \\tau_1  = 0.0625 + 1 * 45.678 = 45.7405$$\n",
    "\n",
    "h. Draw a sample from this updated posterior as $\\mu_2$. Let us assume that this is <mark>10.0266</mark>\n",
    "\n",
    "i. Update the posterior for $\\tau$. Note that the value of \\\\(\\alpha_{posterior}\\\\) does not change.\n",
    "\n",
    "$$\\alpha_{posterior} = \\alpha_{prior} + n/2 = 25 + 1/2 = 25.5$$ \n",
    "\n",
    "$$\\beta_{posterior} = \\beta_{prior} + \\sum_i (x_i - \\mu_{2})^2 / 2 = 0.5 + (10.2 - 10.0266)^2 / 2 = 0.5150$$\n",
    "\n",
    "j. Draw a sample from this posterior distribution as $\\tau_2$. Trial 2 is now complete.\n",
    "\n",
    "k. By now you must have a sense of this process. If not,we simply repeat steps (f)\n",
    " to (j) till we have a sufficient number of samples.\n",
    "\n",
    "<hr style = \"border:1px dotted salmon\"></hr>\n",
    "\n",
    "#### Building the Inferred Distribution\n",
    "\n",
    "Use the current values that we obtain at each step for both $\\mu$ and $\\tau$, and build a frequency distribution (histogram) from it. We can also create a joint distribution as well using a two-dimensional histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNGRADED EVALUATION (2 hours)\n",
    "\n",
    "#### 1. Use the Metropolis Python code as boilerplate code to perform Gibbs Sampling\n",
    "\n",
    "#### 2. Using the parameter values from the example above \n",
    "\n",
    "1. Run a simulation for 1000 iterations. \n",
    "\n",
    "2. Run the simulation for 10 iterations annd print out the following as a table, each row representing a trial\n",
    "\n",
    "   a. the posterior parameter values for both $\\mu$ and $\\tau$ at each trial\n",
    "\n",
    "   b. sampled parameter values $\\mu$ and $\\tau$ at each trial\n",
    "\n",
    "#### 3. Summarize the above distribution - Mean, Variance, Minimum and Maximum, Quartiles\n",
    "\n",
    "#### 4. Plot the joint distribution of the two parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte Carlo (also called Hybrid Monte Carlo)\n",
    "\n",
    "The best resource on the topic! - \n",
    "\n",
    "[Betancourt Youtube Video](https://www.youtube.com/watch?v=VnNdhsm0rJQ)\n",
    "\n",
    "[![Betancourt Youtube Video](https://statmodeling.stat.columbia.edu/wp-content/uploads/2016/06/Screen-Shot-2016-06-10-at-5.29.51-PM.png)](https://www.youtube.com/watch?v=VnNdhsm0rJQ \"Betancourt\")\n",
    "\n",
    "[Stan page on HMC](https://mc-stan.org/docs/2_21/reference-manual/hamiltonian-monte-carlo.html)\n",
    "\n",
    "[Wikipedia Reference](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)\n",
    "\n",
    "\n",
    "We won't go into a lot of detail of HMC here since this is quite conceptually involved. \n",
    "This is based on the solution of differential equations known as Hamilton's equations for the motion of a particle in space. This relates the position of the particle \\\\(x\\\\), the momentum \\\\(m\\\\) and the Hamiltonian $H$ through the following equations \n",
    "\n",
    "$$\\dfrac{dx}{dt} = \\dfrac{dH}{dm}$$\n",
    "\n",
    "$$\\dfrac{dm}{dt} = - \\dfrac{dH}{dx}$$\n",
    "\n",
    "\n",
    "These differential equations depend on the probability distributions we are trying to learn. We navigate these distributions by moving around them in a trajectory using steps that are defined by the position and momentum at that position. Like in many algorithms, the momentum term allows the particle to move up a posterior space, as opposed to always moving down. The steps that we take also depend on the curvature of the posterior. Navigating these trajectories can be a very expensive process and the goal is to minimize this computational process.\n",
    "\n",
    "HMC is based on the notion of conservation of energy. The Hamiltonian is intuitively the sum of the kinetic and potential energy of the particle, or in simple terms it measures the total energy of the system. \n",
    "\n",
    "$$H(x,m) = U(x) + KE(m)$$\n",
    "\n",
    "where \\\\(U(x)\\\\) is the potential energy of the system and \\\\(KE(m)\\\\) is the kinetic energy of the system. The potential energy is measured using the negative log density of the posterior distribution.  When the sampler trajectory is far away from the probability mass center, it has high potential energy but low kinetic energy. When the trajectory is closer to the center of the probability mass it will have high kinetic energy but low potential energy. The kinetic energy term (momentum) involves a mass matrix \\\\(\\Sigma\\\\) that is also the covariance of the normal distribution from which we randomly draw a momentum value \\\\(m\\\\) in our Monte Carlo process. An outline of the steps involved in this algorithm is given below.\n",
    "\n",
    "##### Outline \n",
    "\n",
    "* We start from an initial position \\\\(x_0\\\\). \n",
    "\n",
    "* At each step, we select a random value for momentum from a proposal distribution. This is usually a normal distribution such that\n",
    "\n",
    "$$m \\sim N(\\mu, \\Sigma)$$\n",
    "\n",
    "* From the current position and using the sampled value for momentum, we run the particle for time $L \\cdot \\Delta t$ using a leapfrog integrator which is a numerical integration scheme to march forward in time. The terms $\\Delta t$ refers to the time step taken for the integrator, and $L$ refers to the total number of steps taken. $L$ is a hyperparameter that needs to be tuned carefully. If we are at a spatial location indicated by step \\\\(n\\\\), we start from time 0 (integration time) and integrate till time \\\\(t\\\\) to get the following\n",
    "\n",
    "$$x_n(0) \\longrightarrow x_n(L \\Delta t)$$\n",
    "\n",
    "$$m_n(0) \\longrightarrow m_n(L \\Delta t)$$\n",
    "\n",
    "* The leapfrog integration introduces errors due to the fact that it is a numerical integration method and not an exact integral. This is corrected using a Metropolis-Hastings step that probabilistically accepts the new values of $x_{n+1}$ as $x_n(L \\cdot \\Delta t)$ or the original location $x_n(0)$. The acceptance probability used here is given below. Here \\\\(p(x_n(L \\cdot \\Delta t))\\\\) corresponds to the posterior probability density at the end of the integration scheme and \\\\(p(x_n(0))\\\\) coresponds to the posterior probability density at the beginning of the integration scheme. Also, \\\\(q(m)\\\\) is the probability density of the proposal distribution for the momentum. \n",
    "\n",
    "$$ acceptance \\; rate = \\dfrac{p(x_n(L \\cdot \\Delta t))}{p(x_n(0))} \\times \\dfrac{q(m(L \\cdot \\Delta t))}{q(m(0))}$$\n",
    "\n",
    "* Draw a random value \\\\(u\\\\) from a uniform distribution \\\\(U[0,1]\\\\). If \\\\(r > u\\\\), move to the location \\\\(x_n(L \\cdot \\Delta t)\\\\) otherwise remain at location \\\\(x_n(0)\\\\).\n",
    "\n",
    "* Record the new position \\\\(x_{n+1}\\\\). This is repeated for 'n' or a number of spatial steps. \n",
    "\n",
    "\n",
    "##### Impact of \\\\(T\\\\) in HMC\n",
    "\n",
    "Now \\\\(T\\\\) can be defined such that\n",
    "\n",
    "$$T = L \\cdot \\Delta t$$\n",
    "\n",
    "When there are divergences the sampling process that happens in regions of high curvature, we might have to resort to smaller values of \\\\(\\Delta t\\\\).\n",
    "\n",
    "The use of larger than desirable values for \\\\(T = L \\cdot \\Delta t\\\\) results in the sampler making U-turns at locations of high curvature in the posterior space. This can be wasteful since the sampler then ends up wasting time retracing its steps. One way to mitigate this is through a preliminary run of tuning samples to heuristically select values of  \\\\(T\\\\) that work better.\n",
    "\n",
    "A No U-Turn sampler (NUTS) is an extension of the HMC where the number of steps of the integrator $L$ and therefore \\\\(T\\\\) is automatically tuned. For regions of high curvature a smaller value of \\\\(T\\\\) is used so as to minimize U-turns whereas for flatter regions, a larger \\\\(T\\\\) is used to move faster. Therefore an adaptive \\\\(T\\\\) is used that is locally optimal as opposed to a single value of \\\\(T\\\\). Even though we don't know what the posterior looks like (since this is what we are inferring and therefore we can't evaluate its curvature), the NUTS algorithm has ways to estimate it.\n",
    "\n",
    "NUTS uses a scaling matrix that defines the shape of the sampling distribution through the covariance matrix so that the jumps are bounded in all directions. Poor choice of this scaling matrix can result in the sampling stopping or stalling. Fortunately, tools such as PyMC3 can automatically determine appropriate parameter values during the tuning phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of MCMC\n",
    "\n",
    "#### Representativeness\n",
    "\n",
    "The samples from the MCMC process should be representative of the posterior distribution, it should cover the distribution space thoroughly. The final state of the inferred distribution should be independent of the initial value.\n",
    "\n",
    "There are two ways to measure if your inferred distribution is representative of the true distribution: \n",
    "\n",
    "1. Visual inspection using a trace for convergence\n",
    "\n",
    "2. Numerical measures for convergence\n",
    "\n",
    "The trace is simply the plot of the sample value on the y-axis against the iteration number, when it is sampled, on the x-axis. The first 'n' samples are discarded because the sampling process is moving around in space trying to find the regions of representative posterior density. These 'n' samples are called the burn-in and the samples are usually discarded. The choice of 'n' depends on the distribution but usually around 500 is selected to be an adequate number of samples. As shown below, there is good overlap between the samples from the different chains as indicated by the trace and the density plots.\n",
    "\n",
    "Numerical measures include the Gelman-Rubin statistic (also called the potential scale reduction factor or the shrink factor). This measures the ratio of the variance of the samples among the chains to the variance within the chains. A number greater than 1 usually indicates a lack of convergence.\n",
    "\n",
    "![Trace plot](https://srijithr.gitlab.io/image-20201207170920197.png)\n",
    "\n",
    "<center> Trace plot </center>\n",
    "\n",
    "#### Accurate\n",
    "\n",
    "The samples should be sufficiently large such that the estimates are stable. For this reason, separate 'chains' of samples are run to ensure consistency of results. If the chains vary a lot, as seen by inspecting a histogram or a density plot of the samples, the samples are deemed to not be stable and this requires further investigation. Autocorrelation is a common issue where samples that are drawn are not completely independent of each other. An effective sample size is calculated that gives you the true number of samples that are useful for constructing a distribution.\n",
    "\n",
    "Another measure of accuracy is the Monte Carlo Standard Error (MCSE). If we draw 4 chains, chances are that the sample mean each time will differ from each other and the true mean. The MCSE is simply the standard error of this computed mean. The larger the sample size, the less the MCSE will be.\n",
    "\n",
    "#### Efficiency\n",
    "\n",
    "The samples spanning the distribution should be generated efficiently such that sharper regions are resolved appropriately. There is quite a bit of difference in runtimes between the least and the most efficient MCMC algorithms. For more complex models where the dimensionality of the parameters increase, vanilla Metropolis algorithms can be downright impractical.\n",
    "\n",
    "Run multiple chains in parallel as much as possible since each chain is an embarassingly parallel task. Knowledge of the problem at hand helps one to pick the right sampling algorithm, resulting in more efficient sampling. \n",
    "\n",
    "##### Mean-center the data\n",
    "\n",
    "It is advantageous to mean-center the data before performing MCMC sampling. The example below of linear regression helps to illustrate this. The figure below is a 1-dimensional linear regression problem given by\n",
    "\n",
    "$$ y = \\alpha x + \\beta $$\n",
    "\n",
    "and the class of regression lines along with the associated uncertainty is reflected by the spread of these lines. Here \\\\(\\alpha\\\\) is the slope of the line and \\\\(\\beta\\\\) is the y-intercept. The lines pass around the mean of the x and y values of the data and rotate around this 'pivot point'. For each line, as the slope increases, the y-intercept decreases and vice-versa. Hence there is a strong correlation between these coefficients.\n",
    "\n",
    "![Regression lines](https://courseraimages.s3-us-west-2.amazonaws.com/family_regression.png)\n",
    "\n",
    "<center> Family of Regression lines </center>\n",
    "\n",
    "The figure beneath illustates this correlation between the coefficients of linear regression \\\\(\\alpha\\\\) and \\\\(\\beta\\\\). The distribution space represented there has a very narrow diagonal shape which is not ideal for sampling. If you think about the example of Gibbs sampling, you select one parameter \\\\(\\alpha\\\\) and then search for the other parameter \\\\(\\beta\\\\) by moving along the line parallel to the y axis located at that value of \\\\(\\alpha\\\\). This has a limited range because of this narrow shape of the parameter space as indicated by the correlation plot. Convergence, as a result, can take a long time. \n",
    "\n",
    "When you mean-center the data, you subtract the mean of x from all the values which results in the x values being centered around zero. The pivot point (not exactly at the mean but close to it) is now almost over zero on the x axis which means that the y-intercept does not change much. This breaks the correlation between the two coordinates making it easier to sample.\n",
    "\n",
    "![Inverse correlation](https://courseraimages.s3-us-west-2.amazonaws.com/inverse_correlation.png)\n",
    "\n",
    "<center> Inverse correlation of the intercepts </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADED EVALUATION (30 mins)\n",
    "\n",
    "\n",
    "1. In a Gibbs Sampler, the proposals are always accepted\n",
    "\n",
    "    a. True\n",
    "    \n",
    "    b. False \n",
    "\n",
    "\n",
    "2. A Gibbs Sampler is a specific case of a Metropolis algorithm\n",
    "\n",
    "    a. True\n",
    "    \n",
    "    b. False\n",
    "    \n",
    "    \n",
    "3. Gibbs sampler samples from one parameter at a time, cycling through one parameter at a time.\n",
    "\n",
    "    a. True\n",
    "    \n",
    "    b. False \n",
    "    \n",
    "    \n",
    "4. In Gibbs sampling, the proposal distribution is \n",
    "\n",
    "    a. A Normal distribution\n",
    "    \n",
    "    b. The posterior conditional distribution\n",
    "    \n",
    "    \n",
    "5. We visually inspect the trace to\n",
    "\n",
    "    a. Check for convergence\n",
    "    \n",
    "    b. Determine the largest sampled value\n",
    "    \n",
    "\n",
    "6. We can use a histogram to look at the distribution of the posterior from Metropolis, Metropolis-Hastings or Gibbs sampling\n",
    "\n",
    "   a. True\n",
    "\n",
    "   b. False \n",
    "\n",
    "\n",
    "7. HMC is based on the motion of a particle in space \n",
    "\n",
    "   a. True\n",
    "\n",
    "   b. False\n",
    "\n",
    "\n",
    "8. In HMC, a numerical integration step is performed at each step to march forward and obtain the solution\n",
    "\n",
    "   a. True\n",
    "\n",
    "   b. False\n",
    "\n",
    "\n",
    "9. The reason for a Metropolis-Hastings step when performing HMC is to\n",
    "\n",
    "   a. Make HMC run faster\n",
    "\n",
    "   b. Correct the errors from the numerical integration scheme \n",
    "\n",
    "\n",
    "10. When using NUTS, the number of steps 'L' is automatically tuned\n",
    "\n",
    "   a. True\n",
    "\n",
    "   b. False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
