{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI for Medicine Course 3 Week 2 lecture exercises - Preparing Input for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture notebook we'll be working with input for text classification models. We'll simulate [BERT's](https://github.com/google-research/bert) tokenizer for a simple example. You will use it for real in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the following situation. We have a passage containing medical information of a patient and we would like our model to be able to answer questions using information from this passage. First we'll need to reformulate this question and passage in a way that BERT can read correctly. Let's define a question and a passage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"How old is the patient?\"\n",
    "p = '''\n",
    "The patient is a 64 year old male named Bob. \n",
    "He has no history of chronic spine conditions but is \n",
    "showing mild degenerative changes in the lumbar spine and old right rib fractures.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having these information, we will normally use BERT's tokenizer to tokenize these sentences like this: \n",
    "```python\n",
    "tokenizer.tokenize(q)\n",
    "```\n",
    "\n",
    "Luckily this has been taken care of for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tokens = ['How', 'old', 'is', 'the', 'patient', '?']\n",
    "p_tokens = ['The', 'patient', 'is', 'a', '64', 'year', 'old', 'male', 'named', 'Bob', '.', \n",
    "            'He', 'has', 'no', 'history', 'of', 'chronic', 'spine', 'conditions', 'but', 'is', \n",
    "            'showing', 'mild', 'de', '##gene', '##rative', 'changes', 'in', 'the', 'l', '##umba', \n",
    "            '##r', 'spine', 'and', 'old', 'right', 'rib', 'fracture', '##s', '.']\n",
    "classification_token = '[CLS]'\n",
    "separator_token = '[SEP]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification and separator token are also provided. These tokens can be accessed using the tokenizer as well:\n",
    "```python\n",
    "CLS = tokenizer.cls_token\n",
    "SEP = tokenizer.sep_token\n",
    "```\n",
    "These tokens are really important because we'll need to combinate the question and passage tokens into a single list of tokens and these special tokens allow BERT to understand what is what.\n",
    "\n",
    "The CLS or classification token should come in first. And the SEP token should be used as a separator between the question and the passage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token list looks like this: \n",
      "\n",
      "['[CLS]', 'How', 'old', 'is', 'the', 'patient', '?', '[SEP]', 'The', 'patient', 'is', 'a', '64', 'year', 'old', 'male', 'named', 'Bob', '.', 'He', 'has', 'no', 'history', 'of', 'chronic', 'spine', 'conditions', 'but', 'is', 'showing', 'mild', 'de', '##gene', '##rative', 'changes', 'in', 'the', 'l', '##umba', '##r', 'spine', 'and', 'old', 'right', 'rib', 'fracture', '##s', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "tokens.append(classification_token)\n",
    "tokens.extend(q_tokens)\n",
    "tokens.append(separator_token)\n",
    "tokens.extend(p_tokens)\n",
    "print(f\"The token list looks like this: \\n\\n{tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the complete token list. However we still need to these tokens into numeric representations of themselves. Commonly this is done like this:\n",
    "```python\n",
    "tokenizer.convert_tokens_to_ids(tokens)\n",
    "```\n",
    "This is also provided for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [101, 1731, 1385, 1110, 1103, 5351, 136, 102, 1109, 5351, 1110, 170, 3324, \n",
    " 1214, 1385, 2581, 1417, 3162, 119, 1124, 1144, 1185, 1607, 1104, 13306, 8340, \n",
    " 2975, 1133, 1110, 4000, 10496, 1260, 27054, 15306, 2607, 1107, 1103, 181, 25509, \n",
    " 1197, 8340, 1105, 1385, 1268, 23298, 22869, 1116, 119]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, except the length of the list of token ids will depend on the number of words in the question and the passage and BERT only accepts fixed size input.\n",
    "\n",
    "For this we use **padding**, which means filling out the rest of this list with an empty value until it reaches a maximum length. In this case we'll use \"0\" as our empty value, 60 as the maximum length and we'll leverage the pad_sequences() function from Keras' Sequence module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1731,  1385,  1110,  1103,  5351,   136,   102,  1109,\n",
       "         5351,  1110,   170,  3324,  1214,  1385,  2581,  1417,  3162,\n",
       "          119,  1124,  1144,  1185,  1607,  1104, 13306,  8340,  2975,\n",
       "         1133,  1110,  4000, 10496,  1260, 27054, 15306,  2607,  1107,\n",
       "         1103,   181, 25509,  1197,  8340,  1105,  1385,  1268, 23298,\n",
       "        22869,  1116,   119,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 60\n",
    "\n",
    "token_ids = pad_sequences([token_ids], padding=\"post\", maxlen=max_length)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's like the padding has been done correctly. Commonly this list of token ids is expected to be a Tensor. Luckily we can cast it easily using the convert_to_tensor() function from Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=185, shape=(1, 60), dtype=int32, numpy=\n",
       "array([[  101,  1731,  1385,  1110,  1103,  5351,   136,   102,  1109,\n",
       "         5351,  1110,   170,  3324,  1214,  1385,  2581,  1417,  3162,\n",
       "          119,  1124,  1144,  1185,  1607,  1104, 13306,  8340,  2975,\n",
       "         1133,  1110,  4000, 10496,  1260, 27054, 15306,  2607,  1107,\n",
       "         1103,   181, 25509,  1197,  8340,  1105,  1385,  1268, 23298,\n",
       "        22869,  1116,   119,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tf.convert_to_tensor(token_ids)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done. BERT still needs an input mask as one of its inputs. An input mask is just a list of the same length as the token ids list, indicating if a certain position contains a token or empty values created when padding.\n",
    "\n",
    "We'll showcase how to do this using Keras' Masking layer, but it can be achieved in a simpler way (for this case) with some plain Python. \n",
    "\n",
    "If you're interested in learning some of the details of padding and masking you should check out [this](https://www.tensorflow.org/guide/keras/masking_and_padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=749, shape=(1, 60), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False]])>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "masking_layer = layers.Masking()\n",
    "\n",
    "unmasked = tf.cast(\n",
    "    tf.tile(tf.expand_dims(tf.convert_to_tensor(token_ids), axis=-1), [1, 1, 1]),\n",
    "    tf.float32)\n",
    "\n",
    "masked = masking_layer(unmasked)\n",
    "token_mask = masked._keras_mask\n",
    "token_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the token mask outputs True for tokens and False for padding.\n",
    "\n",
    "Now we have successfully created and formatted the inputs necessary to use the BERT model.\n",
    "\n",
    "In the graded assignment there will be a task very similar to this one, **but with some changes**. It is recommended to create the token ids and the token mask (input_ids and input_mask in the assignment) at the same time using plain Python lists. The reason for this is that the token mask will need to have a different structure than the one the Masking layer produces and also its type should be different.\n",
    "\n",
    "Before ending let's convert the an already padded token ids list to the same type as the one we just did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_token_ids = [  101,  1731,  1385,  1110,  1103,  5351,   136,   102,  1109,\n",
    "                     5351,  1110,   170,  3324,  1214,  1385,  2581,  1417,  3162,\n",
    "                      119,  1124,  1144,  1185,  1607,  1104, 13306,  8340,  2975,\n",
    "                     1133,  1110,  4000, 10496,  1260, 27054, 15306,  2607,  1107,\n",
    "                     1103,   181, 25509,  1197,  8340,  1105,  1385,  1268, 23298,\n",
    "                    22869,  1116,   119,     0,     0,     0,     0,     0,     0,\n",
    "                    0,     0,     0,     0,     0,     0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's convert the list into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=755, shape=(60,), dtype=int32, numpy=\n",
       "array([  101,  1731,  1385,  1110,  1103,  5351,   136,   102,  1109,\n",
       "        5351,  1110,   170,  3324,  1214,  1385,  2581,  1417,  3162,\n",
       "         119,  1124,  1144,  1185,  1607,  1104, 13306,  8340,  2975,\n",
       "        1133,  1110,  4000, 10496,  1260, 27054, 15306,  2607,  1107,\n",
       "        1103,   181, 25509,  1197,  8340,  1105,  1385,  1268, 23298,\n",
       "       22869,  1116,   119,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0], dtype=int32)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_token_ids = tf.convert_to_tensor(padded_token_ids)\n",
    "padded_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the shape of this tensor does not match the desired one. We can easily check this doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_token_ids.shape == token_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the expand_dims() function from Tensorflow we can reshape this tensor into the desired shape, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=757, shape=(1, 60), dtype=int32, numpy=\n",
       "array([[  101,  1731,  1385,  1110,  1103,  5351,   136,   102,  1109,\n",
       "         5351,  1110,   170,  3324,  1214,  1385,  2581,  1417,  3162,\n",
       "          119,  1124,  1144,  1185,  1607,  1104, 13306,  8340,  2975,\n",
       "         1133,  1110,  4000, 10496,  1260, 27054, 15306,  2607,  1107,\n",
       "         1103,   181, 25509,  1197,  8340,  1105,  1385,  1268, 23298,\n",
       "        22869,  1116,   119,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_token_ids = tf.expand_dims(padded_token_ids, 0)\n",
    "padded_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_token_ids.shape == token_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are truly done! **Congratulations on finishing this lecture notebook!!!** Now you should be more familiar with preparing input for BERT. Good job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
