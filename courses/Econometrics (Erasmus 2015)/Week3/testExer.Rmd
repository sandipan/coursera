Test Exercise 3: Answers to the Questions
========================================================

* (a) Use *general-to-specific* to come to a model. Start by regressing the *federal funds rate* on the other \(7\) variables
and eliminate \(1\) variable at a time.

+
```{r echo=FALSE, warning=FALSE}
setwd('C:\\courses\\Coursera\\Current\\Econometrics\\Week3')
library(grid)
#library(gridBase)
library(gridExtra)
df <- read.csv('TestExer 3-TaylorRule-round1.csv')
years <- as.integer(substring(df$OBS,1,4))
df <- df[,-1]

m <- lm(INTRATE~INFL+PROD+UNEMPL+COMMPRI+PCE+PERSINC+HOUST, data=df)
m.sum <- summary(m)
res <- rbind(round(m.sum$coef[,1],4), round(m.sum$coef[,4],4), round(m.sum$r.squared,3))
rownames(res) <- c('coeff', 'p.val', 'R^2')
#grob <-  tableGrob(res)  
grid.newpage()
grid.table(res)
#which.max(res[2,2:ncol(res)])

m <- lm(INTRATE~INFL+PROD+COMMPRI+PCE+PERSINC+HOUST, data=df)
m.sum <- summary(m)
res <- rbind(round(m.sum$coef[,1],4), round(m.sum$coef[,4],4), round(m.sum$r.squared,3))
rownames(res) <- c('coeff', 'p.val', 'R^2')
#g <- tableGrob(res)
grid.newpage()
grid.table(res)
#grid.draw(g)
#which.max(res[2,2:ncol(res)])

m <- lm(INTRATE~INFL+COMMPRI+PCE+PERSINC+HOUST, data=df)
m.sum <- summary(m)
res <- rbind(round(m.sum$coef[,1],4), round(m.sum$coef[,4],4), round(m.sum$r.squared,3))
rownames(res) <- c('coeff', 'p.val', 'R^2')
#g <- tableGrob(res)
grid.newpage()
grid.table(res)
#grid.draw(g)
#which.max(res[2,2:ncol(res)])

print(m.sum)

full <- lm(INTRATE~., data=df)
step(full, data=df, direction="backward")

step(full, data=df, direction="backward", k=log(nrow(df)))

restricted <- lm(INTRATE~INFL+COMMPRI+PCE+PERSINC+HOUST, data=df)
unrestricted <- full

n <- nrow(df)
unrestricted.sse <- sum(resid(unrestricted) ^2)

#n + n * log(2*pi) + n*log(unrestricted.sse/n) + log(n) * (6+1) # BIC
#n + n * log(2*pi) + n*log(unrestricted.sse/n) + 2 * (6+1) # AIC

res <- as.data.frame(matrix(c(summary(unrestricted)$r.squared, AIC(unrestricted, k=2), BIC(unrestricted)), nrow=3)) #AIC(unrestricted, k=log(nrow(df)))
S1 <- sum(resid(unrestricted) ^2)

res <- cbind.data.frame(res, matrix(c(summary(restricted)$r.squared, AIC(restricted, k=2), BIC(restricted)), nrow=3)) #AIC(restricted, k=log(nrow(df)))
S0 <- sum(resid(restricted) ^2)

res <- as.data.frame(sapply(res, function(x){round(x, 5)}))
 
names(res) <- c("unrestricted", "restricted")
rownames(res) <- c("R^2", "AIC", "BIC")

grid.newpage()
grid.table(res)

g <- 2
k <- 8
#F <- ((S0-S1)/g)/(S1/(n-k))
F <- ((summary(unrestricted)$r.squared-summary(restricted)$r.squared)/g)/((1-summary(unrestricted)$r.squared)/(n-k))
print(F)
#1-pf(F, g, n-k) # 2-sided test p value
library(lmtest)
waldtest(unrestricted, restricted)
anova(unrestricted, restricted)


library(leaps)
leaps=regsubsets(INTRATE~INFL+PROD+UNEMPL+PCE+PERSINC+HOUST, data=df, nbest=10, method='backward')
plot(leaps, scale="bic")
```
 

* (b) Use *specific-to-general* to come to a model. Start by regressing the *federal funds rate* on only a constant and
add \(1\) variable at a time. Is the model the same as in \((a)\)?

+
```{r echo=FALSE, warning=FALSE}
res <- as.data.frame(matrix(c(summary(lm(INTRATE~INFL, data=df))$coef[,1], summary(lm(INTRATE~INFL, data=df))$coef[,4], summary(lm(INTRATE~INFL, data=df))$r.squared), nrow=5))
res <- cbind.data.frame(res, matrix(c(summary(lm(INTRATE~PROD, data=df))$coef[,1], summary(lm(INTRATE~PROD, data=df))$coef[,4], summary(lm(INTRATE~INFL, data=df))$r.squared), nrow=5))
res <- cbind.data.frame(res, matrix(c(summary(lm(INTRATE~UNEMPL, data=df))$coef[,1], summary(lm(INTRATE~UNEMPL, data=df))$coef[,4], summary(lm(INTRATE~INFL, data=df))$r.squared), nrow=5))
res <- cbind.data.frame(res, matrix(c(summary(lm(INTRATE~COMMPRI, data=df))$coef[,1], summary(lm(INTRATE~COMMPRI, data=df))$coef[,4], summary(lm(INTRATE~INFL, data=df))$r.squared), nrow=5))
res <- cbind.data.frame(res,  matrix(c(summary(lm(INTRATE~PCE, data=df))$coef[,1], summary(lm(INTRATE~PCE, data=df))$coef[,4], summary(lm(INTRATE~INFL, data=df))$r.squared), nrow=5))
res <- cbind.data.frame(res, matrix(c(summary(lm(INTRATE~PERSINC, data=df))$coef[,1], summary(lm(INTRATE~PERSINC, data=df))$coef[,4], summary(lm(INTRATE~INFL, data=df))$r.squared), nrow=5))
res <- cbind.data.frame(res, matrix(c(summary(lm(INTRATE~HOUST, data=df))$coef[,1], summary(lm(INTRATE~HOUST, data=df))$coef[,4], summary(lm(INTRATE~INFL, data=df))$r.squared), nrow=5))
#res <- as.data.frame(sapply(res, function(x){round(x, 5)}))
names(res) <- names(df)[-1]
rownames(res) <- c("intercept", "coef", "intercept.pval", "coef.pval", "R^2")
grid.newpage()
grid.table(res)

null <- lm(INTRATE~1, data=df)
step(null, data=df, scope=list(lower=null, upper=full), direction="forward")
step(null, data=df, scope=list(lower=null, upper=full), direction="forward", k=log(nrow(df)))
#leaps=regsubsets(INTRATE~., data=df, nbest=10, method='forward')
#plot(leaps, scale="bic")
```

As can be seen from above, 

the **unrestricted model** is \(INTRATE=X_1\beta_1+X_2\beta_2+\epsilon\) and 

the **restricted model** is \(INTRATE=X_1\beta_1+\epsilon\), 

where \(X_1=X(INFL,COMMPRI,PCE,PERSINC,HOUST)\) and \(X_2=X(PROD,UNEMPL)\). 

*Sum of square residual* for the unrestricted model = \(3119.796\), whereas the *Sum of square residual* for the restricted model = \(3129.795\). 

Now the \(F-statistic\) for our hypothesis test \(H_0:\beta_2=0\) against \(H_1:\beta_2 \ne 0\) is \(F = \frac{(R_1^2-R_0^2)/g}{(1-R_1^2)/(n-k)}=\frac{(0.6386-0.63744)/2}{(1-0.6386)/(660-8)}=1.046375\), and the *p-value* is \(0.3517967\) with degrees of freedoms \(2, 653\). Similar results we obtain from **wald test** and **anova**, so we can't reject our **null hypothesis** at 5% significance level, it confirms that we shall be better off without the extra variables (with the **restricted model**).
 
* (c) Compare your model from \((a)\) and the *Taylor rule* of equation \(i_t=\beta_1+\beta_2\pi_t+\beta_3y_t + \epsilon_t\). Consider \(R^2\), *AIC* and *BIC*. Which of the models do you prefer?

+ 
```{r echo=FALSE, warning=FALSE}

mt <- lm(INTRATE~INFL+PROD, data=df)
m.sum <- summary(m)
res <- rbind(round(m.sum$coef[,1],4), round(m.sum$coef[,4],4), round(m.sum$r.squared,3))
rownames(res) <- c('coeff', 'p.val', 'R^2')
#g <- tableGrob(res)
grid.newpage()
grid.table(res)
#grid.draw(g)
#which.max(res[2,2:ncol(res)])
print(m.sum)


res <- as.data.frame(matrix(c(summary(mt)$r.squared, AIC(mt, k=2), BIC(mt)), nrow=3)) #AIC(unrestricted, k=log(nrow(df)))
S0 <- sum(resid(mt) ^2)

res <- cbind.data.frame(res, matrix(c(summary(unrestricted)$r.squared, AIC(unrestricted, k=2), BIC(unrestricted)), nrow=3)) #AIC(restricted, k=log(nrow(df)))
S1 <- sum(resid(unrestricted) ^2)

res <- cbind.data.frame(res, matrix(c(summary(restricted)$r.squared, AIC(restricted, k=2), BIC(restricted)), nrow=3)) #AIC(restricted, k=log(nrow(df)))

res <- as.data.frame(sapply(res, function(x){round(x, 5)}))

names(res) <- c("Taylors (restricted)", "unrestricted", "restricted")
rownames(res) <- c("R^2", "AIC", "BIC")
 
grid.newpage()
grid.table(res)

g <- 5
k <- 8
F <- ((S0-S1)/g)/(S1/(n-k))
F <- ((summary(unrestricted)$r.squared-summary(mt)$r.squared)/g)/((1-summary(unrestricted)$r.squared)/(n-k))
print(F)
1-pf(F, g, n-k) # 2-sided test p value

waldtest(unrestricted, mt)
anova(unrestricted, mt)

#waldtest(restricted, mt)
anova(restricted, mt)

```
Now the \(F-statistic\) for our hypothesis test to test the Taylor's (restricted) model against the unrestricted model, we found \(F = 23.01\), and the *p-value* is \(<0.05\). Similar results we obtain from **wald test** and **anova**, so we can reject our **null hypothesis** at 5% significance level, it confirms that Taylor's model is not better thab the  **unrestricted model**.
 
Similarly with **anova** test we can see that the *p-value* is small, so we can reject the null hypothesis again and conclude that 
Taylor's model is not better thab the  **restricted model** (we can't do **walds test** because these models are not nested).

* (d) Test the *Taylor rule* of equation  \(i_t=\beta_1+\beta_2\pi_t+\beta_3y_t + \epsilon\) using the *RESET test*, *Chow break* and *forecast test* (with in both tests as break date January \(1985\)) and a *Jarque-Bera test*. What do you conclude?

*Sum of square residual* for the unrestricted model = \(S1 = 3119.796\), whereas the *Sum of square residual* for the restricted Taylor's model = \(S0=3670.306\) and the *F-statistic* for the **RESET Test** \(F=\frac{(S_0-S_1)/g}{S_1/(n-k)}=\frac{(3119.796-3670.306)/1}{3119.796/(660-4)}=115.7558\), with *p-value=0*, we can reject the *null hypothesis* at 5% significance level, concluding that *we reject that the model is a  linear regression model*. 

+ 
```{r echo=FALSE, warning=FALSE}

g <- 1
k <- 3 + 1
F <- ((S0-S1)/g)/(S1/(n-k))
print(F)
1-pf(F, g, n-k) # 2-sided test p value

#resettest(INTRATE~INFL+PROD, power=2, type="regressor", data=df)

```

+ 
```{r echo=FALSE, warning=FALSE}
mt <- lm(INTRATE~INFL+PROD, data=df)
S0 <- sum(resid(mt) ^2)
df1 <- df[years<=1984,]
mt1 <- lm(INTRATE~INFL+PROD, data=df1)
S1 <- sum(resid(mt1) ^2)
df2 <- df[years > 1984,]
mt2 <- lm(INTRATE~INFL+PROD, data=df2)
S2 <- sum(resid(mt2) ^2)
k <- 3
F <- ((S0-(S1+S2))/k) /((S1 + S2) / (n - 2*k))
p <- 1-pf(F,k,n-2*k)
n1 <- nrow(df1)
n2 <- nrow(df2)
F <- ((S0-S1)/n2) /(S1/(n1 - k))
p <- 1-pf(F,k,n-2*k)
library(moments)
S=moments::skewness(resid(mt))
K=moments::kurtosis(resid(mt))
JB=(sqrt(n/6)*S)^2+(sqrt(n/24)*(K-3))^2
1-pchisq(JB, 2)
#library(tseries)
#jarque.bera.test(resid(mt))
```

The total *Sum of square residual* for the Taylor's model for *1960:1 - 2014:12* is \(=3670.306\), whereas the *Sum of square residual* before 1985 January (*1960:1 - 1984:12*) \(= S_1 = 1757.802\), whereas the *Sum of square residual* for on or after 1985 January ((*1985:1 - 2014:12*)) \(= S_2=1813.22\). Hence, the *F-statistic* for the **Chow break Test** \(F=\frac{(S_0-(S_1+S_2))/k}{(S_1+S_2)/(n-2*k)}=4.578194\), with *p-value=0.003568257*, we can reject the *null hypothesis* at 5% significance level, concluding that *the model parameters do not suffer from the break*.

The *F-statistic* for the **Chow forecast Test** \(F=\frac{(S_0-S_1)/n_2}{S_1/(n_1-k)}=0.8976073\), with *p-value* as \(0.4422323\), where we have \(n_1=300,\;n_2=360\). The *p-value* is not *significant* at 5% level, hence we can't reject the *null hypothesis* that *there is no structural change in the prediction period*.

The *Jarque-Bera test statistic* is \(JB=\sqrt(\frac{n}{6}S)^2+\sqrt(\frac{n}{24}(K-3))^2=9.171319\), where *skewness* of residuals \(S=0.3213945\) and *kurtosis* \(K=3.164455\), with *p-value* \(0.01019702\) so that we can reject the *null hypothesis* \(H_0\) that residuals are *normally distributed*, at 5% level of significance.